{"cells":[{"cell_type":"markdown","metadata":{"id":"-DCJfD0Iu4Tk"},"source":["# Week 8 - Conversation and Text Generation\n","Many natural language activities boil down to text generation, especially the back-and-forth nature of natural conversation and question answering. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n","\n","\n","Much recent NLP research is on text generation. Before the phenomenal product ChatGPT(OpenAI) was released at the end of 2022,  this is the primary use of large language models like GPT-3 (OpenAI), Wu Dao (Beijing Academy of AI), and Gopher (DeepMind). Then everything went wild in 2023. We saw tons of open-sourced LLMs were released like Llama series(Meta), and how others techs wanted to compete with OpenAI by releasing models like Claude(Anthropic), Bard(Google), Gemini(Google x2), etc. It seems that the era pf artificial general intelligence (AGI) is approaching!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44960,"status":"ok","timestamp":1708203282511,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"UAiiglFku4Tm","outputId":"ae7b3f68-22d0-4ce0-bbcb-96d8f4f1eebf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n","  Cloning https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git to /tmp/pip-req-build-1cyspnx9\n","  Running command git clone --filter=blob:none --quiet https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git /tmp/pip-req-build-1cyspnx9\n","  Resolved https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git to commit b17a265d3b8253424e5b38872457f7437909a65d\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (2.31.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.5.3)\n","Collecting python-docx (from lucem-illud==8.0.1)\n","  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (9.4.0)\n","Collecting pdfminer2 (from lucem-illud==8.0.1)\n","  Downloading pdfminer2-20151206-py2.py3-none-any.whl (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting GitPython (from lucem-illud==8.0.1)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.9.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.11.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (1.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (4.3.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.7.1)\n","Collecting pyanno3 (from lucem-illud==8.0.1)\n","  Downloading pyanno3-2.0.2.tar.gz (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (4.12.3)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.20.1)\n","Collecting boto3 (from lucem-illud==8.0.1)\n","  Downloading boto3-1.34.44-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.2.1)\n","Collecting pydub (from lucem-illud==8.0.1)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting speechrecognition (from lucem-illud==8.0.1)\n","  Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl (32.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pysoundfile (from lucem-illud==8.0.1)\n","  Downloading PySoundFile-0.9.0.post1-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (0.19.3)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (7.34.0)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from lucem-illud==8.0.1) (3.7.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->lucem-illud==8.0.1) (2.5)\n","Collecting botocore<1.35.0,>=1.34.44 (from boto3->lucem-illud==8.0.1)\n","  Downloading botocore-1.34.44-py3-none-any.whl (12.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->lucem-illud==8.0.1)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->lucem-illud==8.0.1)\n","  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->lucem-illud==8.0.1) (6.4.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython->lucem-illud==8.0.1)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (67.7.2)\n","Collecting jedi>=0.16 (from IPython->lucem-illud==8.0.1)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->lucem-illud==8.0.1) (4.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lucem-illud==8.0.1) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->lucem-illud==8.0.1) (4.66.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lucem-illud==8.0.1) (2023.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pdfminer2->lucem-illud==8.0.1) (1.16.0)\n","Collecting traits (from pyanno3->lucem-illud==8.0.1)\n","  Downloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from pysoundfile->lucem-illud==8.0.1) (1.16.0)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->lucem-illud==8.0.1) (4.9.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx->lucem-illud==8.0.1) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lucem-illud==8.0.1) (2024.2.2)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (2024.2.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->lucem-illud==8.0.1) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lucem-illud==8.0.1) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (0.9.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (2.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.1.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->lucem-illud==8.0.1) (3.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud==8.0.1) (2.21)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->lucem-illud==8.0.1)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->lucem-illud==8.0.1) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->lucem-illud==8.0.1) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->lucem-illud==8.0.1) (0.2.13)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->lucem-illud==8.0.1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->lucem-illud==8.0.1) (2.16.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy->lucem-illud==8.0.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy->lucem-illud==8.0.1) (0.1.4)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->lucem-illud==8.0.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->lucem-illud==8.0.1) (2.1.5)\n","Building wheels for collected packages: lucem-illud, pyanno3\n","  Building wheel for lucem-illud (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lucem-illud: filename=lucem_illud-8.0.1-py3-none-any.whl size=34989 sha256=ab0ffdd2aba172f907bdbfc6e94ecf6b061478e00869bbfca78222581ad42260\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ixrhlp2e/wheels/4c/84/7b/d89dec34fb910351cf618bf262d1c926f9f68779ef24028427\n","  Building wheel for pyanno3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyanno3: filename=pyanno3-2.0.2-py3-none-any.whl size=116965 sha256=0c714091a631cd2d78b82afb5bbf13bdc2d03f2b25c3781fb8b67703b33aca94\n","  Stored in directory: /root/.cache/pip/wheels/ce/0c/9f/21212c187c1edb1647c18e0a24b6b213b3bb7dee5aa94bfd72\n","Successfully built lucem-illud pyanno3\n","Installing collected packages: pydub, traits, smmap, python-docx, pdfminer2, jmespath, jedi, speechrecognition, pysoundfile, pyanno3, gitdb, botocore, s3transfer, GitPython, boto3, lucem-illud\n","Successfully installed GitPython-3.1.42 boto3-1.34.44 botocore-1.34.44 gitdb-4.0.11 jedi-0.19.1 jmespath-1.0.1 lucem-illud-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.25.1 pysoundfile-0.9.0.post1 python-docx-1.1.0 s3transfer-0.10.0 smmap-5.0.1 speechrecognition-3.10.1 traits-6.4.3\n"]}],"source":["#Special module written for this class\n","#This provides access to data and to helper functions from previous weeks\n","#Make sure you update it before starting this notebook\n","# !pip install -U git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n","import lucem_illud\n","\n","import sklearn #For generating some matrices\n","import pandas as pd #For DataFrames\n","import numpy as np #For arrays\n","import matplotlib.pyplot as plt #For plotting\n","import seaborn #Makes the plots look nice\n","import seaborn as sns\n","import scipy #Some stats\n","import nltk #a little language code\n","from IPython.display import Image #for pics\n","\n","import pickle #if you want to save layouts\n","import os\n","import io\n","import zipfile\n","\n","import networkx as nx\n","\n","%matplotlib inline\n","\n","import torch # pip install torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertConfig # pip install tranformers\n","from transformers import AdamW, BertForSequenceClassification\n","from tqdm import tqdm, trange"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lk2R1QxOu4Tm"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UsG9ZsWu4Tn"},"outputs":[],"source":["import os\n","os.environ['KERAS_BACKEND'] = 'tensorflow'\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","metadata":{"id":"Jcxh_Xuzu4Tn"},"source":["# ConvoKit\n","As we alluded to in Week 7 with causal inference, [ConvoKit](https://convokit.cornell.edu/) is an exciting platform for conversational analysis developed by Jonathan Chang, Calem Chiam, and others, mostly at Cornell. Keep this in mind if you are interested in a final project with conversational data such as Twitter threads or movie scripts. They have an [interactive tutorial](https://colab.research.google.com/github/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/Introduction_to_ConvoKit.ipynb), which we include some examples from below. Most of the following text and code is authored by them.\n","\n","These ConvoKit corpora can be used for the next exercise in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41960,"status":"ok","timestamp":1708203324447,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"EpQTyQjBUonG","outputId":"7a0add94-542a-4d53-f998-cac0f2c239d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting convokit\n","  Downloading convokit-3.0.0.tar.gz (183 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m163.8/183.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n","Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n","  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n","Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n","Collecting dill>=0.2.9 (from convokit)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n","Collecting clean-text>=0.6.0 (from convokit)\n","  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n","Collecting unidecode>=1.1.1 (from convokit)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.2)\n","Collecting pymongo>=4.0 (from convokit)\n","  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n","Collecting dnspython>=1.16.0 (from convokit)\n","  Downloading dnspython-2.6.0-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.4/307.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.12.25)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.16.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.9.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy>=2.3.5->convokit) (0.1.4)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.3.5->convokit) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n","Building wheels for collected packages: convokit, emoji\n","  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=cbfd522f3e5dc50f8f9a79dc79d16ad73be4aaa3c2f847e7c6d43daa8ea3074f\n","  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=d9eecba30b42016da07d0869c1c4132daee628aa0e1ab07d877e8dedbfb72a2e\n","  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n","Successfully built convokit emoji\n","Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n","Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.0 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.1 unidecode-1.3.8\n"]}],"source":["try:\n","    import convokit\n","except ModuleNotFoundError:\n","    !pip install convokit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMQXf1xHUonH"},"outputs":[],"source":["# for pretty printing of cells within the Colab version of this notebook\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":1273,"status":"ok","timestamp":1708203325695,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"RLHbOOAzUonI","outputId":"6e9a120c-4f66-48ad-9a5a-50eec1aa5ad0"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import convokit\n","from convokit import Corpus, download"]},{"cell_type":"markdown","metadata":{"id":"M5VIPx2oUonI"},"source":["### Loading a Corpus"]},{"cell_type":"markdown","metadata":{"id":"j_lhin9LUonJ"},"source":["A Corpus represents a conversational dataset. We typically begin our analysis by loading a Corpus. A list of existing datasets already in ConvoKit format can be found [here](https://convokit.cornell.edu/documentation/datasets.html).\n","\n","A growing list of many other conversational datasets covering a variety of conversational settings are available in ConvoKit, such as face-to-face (e.g. the [*Intelligence Squared Debates corpus*](https://convokit.cornell.edu/documentation/iq2.html)), institutional (e.g. the [*Supreme Court Oral Arguments corpus*](https://convokit.cornell.edu/documentation/supreme.html)), fictional (e.g. the [*Cornell Movie Dialog Corpus*](https://convokit.cornell.edu/documentation/movie.html)), or online  (e.g. all talkpage conversations on [*Wikipedia Talk Pages*](https://convokit.cornell.edu/documentation/wiki.html) and a full dump of [*Reddit*](https://convokit.cornell.edu/documentation/subreddit.html)).\n","\n","For this tutorial, we will primarily be using the *r/Cornell* subreddit corpus to demo various ConvoKit functionality, and occasionally the [*Switchboard Dialog Act Corpus*](https://convokit.cornell.edu/documentation/switchboard.html) (a collection of anonymized five-minute telephone conversations) as a contrasting dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"elapsed":7254,"status":"ok","timestamp":1708203382483,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"488jR7pVUonJ","outputId":"01f7ccaa-cdd1-4c1a-911c-737c08865438"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading subreddit-Cornell to /root/.convokit/downloads/subreddit-Cornell\n","Downloading subreddit-Cornell from http://zissou.infosci.cornell.edu/convokit/datasets/subreddit-corpus/corpus-zipped/CookingScrewups~-~CrappyDesign/Cornell.corpus.zip (11.2MB)... Done\n","No configuration file found at /root/.convokit/config.yml; writing with contents: \n","# Default Backend Parameters\n","db_host: localhost:27017\n","data_directory: ~/.convokit/saved-corpora\n","default_backend: mem\n"]}],"source":["corpus = Corpus(download('subreddit-Cornell'))\n","\n","# You can try a different corpus if you want.\n","#corpus = Corpus(download('diplomacy-corpus'))\n","#corpus = Corpus(download('switchboard-corpus'))\n","#corpus = Corpus(download('reddit-corpus-small'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1708203389805,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"rnBisd3QUonL","outputId":"097b2ca7-49bd-4426-ebf7-497289d726f7"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Number of Speakers: 7568\n","Number of Utterances: 74467\n","Number of Conversations: 10744\n"]}],"source":["corpus.print_summary_stats()"]},{"cell_type":"markdown","metadata":{"id":"XTcJWCgzUonL"},"source":["### Corpus components: Conversations, Utterances, Speakers"]},{"cell_type":"markdown","metadata":{"id":"MnnTm_ALUonL"},"source":["Every Corpus has three main components: [Conversations](https://convokit.cornell.edu/documentation/conversation.html), [Utterances](https://convokit.cornell.edu/documentation/utterance.html), and [Speakers](https://convokit.cornell.edu/documentation/speaker.html). Just as in real life, in ConvoKit, Conversations are some sequence of Utterances, where each Utterance is made by some Speaker. Let's look at an example of each."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1708203393035,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"xW9eeQrcUonS","outputId":"6a700da0-4f2b-40af-ac52-efdc1eaa5789"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["ConvoKitMeta({'title': 'Cornell Refuses Gift from Renowned Architect Alumnus After 5 Women Say He Sexually Harassed Them', 'num_comments': 8, 'domain': 'cornellsun.com', 'timestamp': 1521000158, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': None, 'stickied': False, 'author_flair_text': ''})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# This is a Reddit thread\n","corpus.random_conversation().meta"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1708203393206,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"TeR71mPgUonT","outputId":"a706a932-6984-4147-8ce9-0ef53f9c10bc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["ConvoKitMeta({'score': 1, 'top_level_comment': 'di08apt', 'retrieved_on': 1496811338, 'gilded': 0, 'gildings': None, 'subreddit': 'Cornell', 'stickied': False, 'permalink': '', 'author_flair_text': '2020'})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# This is a Reddit post or comment.\n","corpus.random_utterance().meta"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1708203393491,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"LnQcLKBxUonS","outputId":"9d067404-f326-4422-f8ef-33def2ebf821"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Speaker({'obj_type': 'speaker', 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x7b18f4e8a3e0>, 'id': 'Important_Lettuce', 'meta': ConvoKitMeta({})})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# The r/Cornell Corpus does not have speaker metadata.\n","#corpus.random_speaker().meta\n","\n","#Speaker do have an 'id' which is their Reddit username, as seen here.\n","corpus.random_speaker()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1708203393621,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"kpfMX5_bu4Tp","outputId":"fbcc7f25-bea3-4642-9b72-aa43fff7a81c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["I was just reading about the Princeton Mic-Check and it's getting [national press](http://www.bloomberg.com/news/2011-12-29/princeton-brews-trouble-for-us-1-percenters-commentary-by-michael-lewis.html).\n","\n","I want to get a sense of what people felt like around campus. Anything interesting happen? Anything interesting coming up?\n"]}],"source":["# We can iterate through these objects as we iterate lists or DataFrames in Python.\n","for utt in corpus.iter_utterances():\n","    print(utt.text)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"iP83knAGu4Tp"},"source":["Conversations, Utterances, and Speakers are each interesting, but the magic of conversational analysis is connecting them. For example, we can get all the Conversations in which a Speaker has participated and all the Utterances they have made. To make it more interesting, we can find a Speaker to study by navigating from a random Utterance."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1708203396748,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"fAyGVo-Ou4Tp","outputId":"2b3de52d-6005-4182-e5a6-2a1c107ef321"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7b18f4e8a3e0>, 'id': 'e27y8', 'meta': ConvoKitMeta({'title': '\"For small creatures such as we the vastness is bearable only through love.\" Happy Carl Sagan Day!', 'num_comments': 1, 'domain': 'youtube.com', 'timestamp': 1289070151, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': None, 'stickied': False, 'author_flair_text': ''})}),\n"," Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7b18f4e8a3e0>, 'id': 'e3v2z', 'meta': ConvoKitMeta({'title': 'Cornell University Lab Releases Powerful New Evidence that the Human Mind can Perceive the Future', 'num_comments': 1, 'domain': 'hplusmagazine.com', 'timestamp': 1289364274, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': None, 'stickied': False, 'author_flair_text': ''})}),\n"," Conversation({'obj_type': 'conversation', 'vectors': [], 'tree': None, 'owner': <convokit.model.corpus.Corpus object at 0x7b18f4e8a3e0>, 'id': 'e4m94', 'meta': ConvoKitMeta({'title': 'Dump and Run Sale Earns $3,500 More for Charity in 2010 | The Cornell Daily Sun', 'num_comments': 0, 'domain': 'cornellsun.com', 'timestamp': 1289489419, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': None, 'stickied': False, 'author_flair_text': ''})})]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# consider this sequence of operations that highlight how to navigate between components\n","utt = corpus.random_utterance()\n","convo = utt.get_conversation() # get the Conversation the Utterance belongs to\n","spkr = utt.speaker # get the Speaker who made the Utterance\n","\n","spkr_convos = list(spkr.iter_conversations())\n","\n","# Display up to 3 of their conversations.\n","spkr_convos[:3]"]},{"cell_type":"markdown","metadata":{"id":"BHkHRCiAu4Tp"},"source":["For a more qualitative feel of the data, you can display a Conversation. For Reddit data, this is a single thread."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1708203399484,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"gCVGO_rZu4Tp","outputId":"7ee436d8-8ee0-45b2-8296-19b81f032be9"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","    Holy crap.  I only glanced at the abstract of the paper, but it says p-value = 1\n","\n"]}],"source":["# We truncate sentences at character 80 to avoid making this notebook too long!\n","convo.print_conversation_structure(lambda utt: utt.text[:80] + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"XJloyysmu4Tp"},"source":["There is a lot more to ConvoKit that we encourage you to explore, especially their [tutorial](https://colab.research.google.com/github/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/Introduction_to_ConvoKit.ipynb), but the ability to seamlessly navigate between the Utterances, Conversations, and Speakers of a Corpus is extremely valuable for social science."]},{"cell_type":"markdown","metadata":{"id":"yB8Il5Fxu4Tp"},"source":["## <font color=\"red\">*Exercise 1*</font>\n","\n","<font color=\"red\">Construct cells immediately below this that use ConvoKit to analyze a Corpus other than 'subreddit-Cornell', including at least one function you find in the package not used above. You can also generate a ConvoKit Corpus from your own dataset based on [their Corpus from .txt files tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/converting_movie_corpus.ipynb) or [their Corpus from pandas tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/corpus_from_pandas.ipynb), but that may be time-consuming for a weekly assignment. (It could be a great idea for your final project!)"]},{"cell_type":"markdown","metadata":{"id":"QTLm96n5u4Tp"},"source":["## Creating networks of agents from corpora\n","\n","Now let's return to the Davies corpora (specifically, Soap Operas) to see how we can extract actors and build a network of their relationships in the texts.\n","\n","We'll use the `lucem_illud.loadDavies()` function to get the dataframe. Make sure to download `SOAP.zip` from DropBox, unzip, and edit the following line with the path to that file. This code may take some time."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16929,"status":"ok","timestamp":1708286674138,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"QUb1szm43BQV","outputId":"ea0a7b79-4505-43bc-8b96-3ece2a71dd2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1708203627565,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"wa5SHZElu4Tq","outputId":"06fcf27a-b2f1-44e3-e14f-1381ba5c9a0d"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["corpora_address = \"/content/drive/MyDrive/Colab Notebooks/Computational Content Analysis/Homework-Notebooks-2024-Winter/data/SOAP\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gAsNS0tu4Tq"},"outputs":[],"source":["soap_texts = lucem_illud.loadDavies(corpora_address, num_files=2000)"]},{"cell_type":"markdown","metadata":{"id":"eLX8EyJ4u4Tq"},"source":["We now use the source to see how the data is stored. Note that this is different from the movies corpus, and that we will need to use a different aggregating method to store the data. Each dataset would have a different approach, but they are all very similar, it depends on how the data is stored. Here multiple textids match multiple scripts, so our soap dataframe would be structured a little differently.\n","\n","You can see the first 20 lines of the source file here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMK-JFuXu4Tq"},"outputs":[],"source":["zfile = zipfile.ZipFile(corpora_address + \"/soap_sources.zip\")\n","source = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMj8Rj8Ou4Tq"},"outputs":[],"source":["for file in zfile.namelist():\n","    with zfile.open(file) as f:\n","        for line in f:\n","            source.append(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WuMMQBQu4Tq"},"outputs":[],"source":["source[0:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ot2kZRB8u4Tq"},"outputs":[],"source":["soap_dict = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3RxxObGu4Tr"},"outputs":[],"source":["for soap in source[3:]:\n","    try:\n","        textID, year, show, url = soap.decode(\"utf-8\").split(\"\\t\")\n","    except UnicodeDecodeError:\n","        continue\n","    if show.strip() not in soap_dict:\n","        soap_dict[show.strip()] = []\n","    if show.strip() in soap_dict:\n","        try:\n","            soap_dict[show.strip()].append(soap_texts[textID.strip()])\n","        except KeyError:\n","            continue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ll0kJ-aru4Tr"},"outputs":[],"source":["soap_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVGLhkScu4Tr"},"outputs":[],"source":["soap_df = pd.DataFrame(columns=[\"Soap Name\", \"Tokenized Texts\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2WG3rpPu4Tr"},"outputs":[],"source":["i = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2SlNnTXu4Ts"},"outputs":[],"source":["for soap in soap_dict:\n","    # since there were multiple lists\n","    print(soap)\n","    full_script = []\n","    for part in soap_dict[soap]:\n","        full_script = full_script + part\n","    soap_df.loc[i] = [soap, full_script]\n","    i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDHoEjZ3u4Ts"},"outputs":[],"source":["soap_df"]},{"cell_type":"markdown","metadata":{"id":"Ed-L10sgu4Ts"},"source":["We now have each Soap, and each of the Tokenized texts. Let us see what kind of information we can get. These are American soap operas, and are likely to be cheesy and dramatic (an understatment). A fun start would be to make networks of each of the actors and actresses in these soaps.\n","\n","What would be a good way to create a network? Maybe everytime someone talks to someone we add one weight? But we wouldn't want to add weights whenever it's a different scene - or maybe we do? Let us look at the text and figure it out.\n","\n","Note that we didn't add the year here because it spans over multiple years. If we are doing different kinds of analysis we would want to a years column as well.\n","\n","In my dataframe, Days of Our Lives is the 4th corpora, and I conducted my basic analysis on that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqbNeFcFu4Ts"},"outputs":[],"source":["dool = soap_df['Tokenized Texts'][3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc0GN_A8u4Ts"},"outputs":[],"source":["' '.join(dool[0:1500])"]},{"cell_type":"markdown","metadata":{"id":"fxRfbaNNu4Tt"},"source":["Hmmm... we can't do our normal text processing. But this provides us with an interesting oppurtunity: every '@!' is followed by some useeful information. Let us do a quick check of how many characters exist here, and how many times they speak."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLMOTu8vu4Tt"},"outputs":[],"source":["characters = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRDRSj4iu4Tt"},"outputs":[],"source":["for token in dool:\n","    if token[0] == '@':\n","        # all characters or actions start with @, so we add that to character\n","        if token[2:] not in characters:\n","            characters[token[2:]] = 0\n","        if token[2:] in characters:\n","            characters[token[2:]] += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTPwcelKu4Tt"},"outputs":[],"source":["len(characters)"]},{"cell_type":"markdown","metadata":{"id":"pMgc_hE2u4Tt"},"source":["Wow, that's a lot of characters: but we notice a '@!' between certain actions too, such as screaming and sobbing. Let us maybe only look for characters with a high number of appearances?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKtIeqYRu4Tu"},"outputs":[],"source":["for character in characters:\n","    if characters[character] > 2000:\n","        print(character, characters[character])"]},{"cell_type":"markdown","metadata":{"id":"blpP9ybfu4Tu"},"source":["Let's check these folks out on the interwebz...a image of search of the name + \"days of our lives\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EcmtStiu4Tu"},"outputs":[],"source":["Image(filename='../data/dool/dool_john.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNL8C-78u4Tu"},"outputs":[],"source":["Image(filename='../data/dool/dool_brady.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"064MPyDFu4Tu"},"outputs":[],"source":["# Image(filename='../data/dool/dool_hope.jpeg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7JYjj1Zu4Tu"},"outputs":[],"source":["# Image(filename='../data/dool/dool_philip.jpeg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xwnbjo9Cu4Tu"},"outputs":[],"source":["# Image(filename='../data/dool/dool_marlena.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxO2hpxHu4Tu"},"outputs":[],"source":["# Image(filename='../data/dool/dool_kate.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w35A6ltFu4Tu"},"outputs":[],"source":["# Image(filename='../data/dool/dool_bo.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydQGsGvpu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_chloe.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP_ZcDwTu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_sami.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UH8aDZHu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_shawn.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i95yDU-Hu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_belle.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxOD67Imu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_lucas.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yC-4NzJxu4Tv"},"outputs":[],"source":["# Image(filename='../data/dool/dool_nicole.jpg')"]},{"cell_type":"markdown","metadata":{"id":"Te4bqgIXu4Tv"},"source":["These are definitely big, long-time players in the dramatic Days narrative. It would make sense to create a graph where each character who appears over 2000 times is a node, and each time they talk to each other, we add one to their weight. We should also store all the things these chracters say: that's useful information.\n","\n","So we now iterate through the tokens in a manner where we can capture this information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w53JQIXou4Tv"},"outputs":[],"source":["actor_network = nx.Graph()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfBhD5bDu4Tv"},"outputs":[],"source":["for character in characters:\n","    if characters[character] > 2000:\n","        actor_network.add_node(character, lines_spoken= characters[character], words=[])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86VTpmOou4Tv"},"outputs":[],"source":["len(actor_network.nodes.data())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t02UF8ZFu4Tv"},"outputs":[],"source":["actor_network.nodes.data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2Ka980gu4Tv"},"outputs":[],"source":["actor_network.nodes['Sami']['lines_spoken']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7sqeWfBu4Tv"},"outputs":[],"source":["i = 0"]},{"cell_type":"markdown","metadata":{"id":"Ue8KIOQxu4Tv"},"source":["The following lines of code creates the graph of actors and their relationships."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_bBu0L_u4Tw"},"outputs":[],"source":["for token in dool:\n","    i += 1\n","    if i > len(dool):\n","        break\n","    if token[0] == \"@\":\n","        if token[2:] in actor_network.nodes():\n","            j = i\n","            for token_ in dool[i:]:\n","                if token_[0] == \"@\":\n","                    # if both the characters exist in the graph, add a weight\n","                    if token_[2:] != token[2:] and token_[2:] in actor_network.nodes():\n","                        if (token[2:], token_[2:]) not in actor_network.edges():\n","                            actor_network.add_edge(token[2:], token_[2:], weight=0)\n","                        if (token[2:], token_[2:]) in actor_network.edges():\n","                            actor_network.edges[(token[2:], token_[2:])]['weight'] += 1\n","                    break\n","                j += 1\n","            # adding characters sentences\n","            actor_network.nodes[token[2:]]['words'].append(dool[i:j])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihe8wrPlu4Tw"},"outputs":[],"source":["nx.draw(actor_network, with_labels=True, font_weight='bold')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgceZkgru4Tw"},"outputs":[],"source":["L = []\n","for node in actor_network.nodes():\n","    l = []\n","    for node_ in actor_network.nodes():\n","        if node == node_:\n","            l.append(0)\n","        else:\n","            l.append(actor_network.edges[(node, node_)]['weight'])\n","    L.append(l)\n","M_ = np.array(L)\n","fig = plt.figure()\n","div = pd.DataFrame(M_, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n","ax = sns.heatmap(div)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxat6VEou4Tw"},"outputs":[],"source":["from networkx.algorithms.community import greedy_modularity_communities\n","c = list(greedy_modularity_communities(actor_network))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BaszQk7u4Tw"},"outputs":[],"source":["c"]},{"cell_type":"markdown","metadata":{"id":"oysBkvRgu4Tw"},"source":["### Finding structure in networks\n","\n","We now have a lot of useful information: we have a graph of all the characters, with their relationships with other characters, as well as all the words they've said. We tried finding communities, but it seems like everyone is connected to everyone: each of them form their own 'community'. Seems like people talk to each other a bunch in soaps.\n","\n","This is however, not the best network to find any meaningful patterns, as we can see with everyone connected to everyone. But as we can see with our heatmap, not everyone talks to everyone an equal amount. How about we only keep our \"important\" ties, where people are talking to each other a lot?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reGmOgWtu4Tw"},"outputs":[],"source":["smaller_actor_network = nx.Graph()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neIvyeZlu4Tw"},"outputs":[],"source":["for actor_1 in actor_network.nodes():\n","    smaller_actor_network.add_node(actor_1, lines_spoken= actor_network.nodes[actor_1]['lines_spoken'], words=actor_network.nodes[actor_1]['words'])\n","    for actor_2 in actor_network.nodes():\n","        if actor_2!=actor_1 and actor_network.edges[(actor_1, actor_2)]['weight'] > 250:\n","            smaller_actor_network.add_edge(actor_1, actor_2, weight=actor_network.edges[(actor_1, actor_2)]['weight'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rn0OHDdnu4Tw"},"outputs":[],"source":["nx.draw(smaller_actor_network, with_labels=True, font_weight='bold')"]},{"cell_type":"markdown","metadata":{"id":"CVgfKUi7u4Tw"},"source":["This is a lot more interesting: while the sets of characters overlap, there is still two distinct communities if you look at characters who regularly talk to each other!\n","\n","Let us see what our centrality measures look like, as well as communities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMMB6zVnu4Tw"},"outputs":[],"source":["from networkx.algorithms.community import greedy_modularity_communities\n","c = list(greedy_modularity_communities(smaller_actor_network))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nn4PJkmTu4Tw"},"outputs":[],"source":["c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QY_7JTpIu4Tw"},"outputs":[],"source":["dcentralities = nx.degree_centrality(smaller_actor_network)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-ERLcACu4Tw"},"outputs":[],"source":["dcentralities['John'], dcentralities['Philip']"]},{"cell_type":"markdown","metadata":{"id":"Me3dpUwnu4Tw"},"source":["Our two different communities show up as detected by the networkx algorithm, and when we look at centralities, we can see that John is a lot more central than Philip."]},{"cell_type":"markdown","metadata":{"id":"T5roxG4Wu4Tx"},"source":["Let us go back to our original graph, and see if the weight or number of similar appearences matches the text... how do we do this? Well, we already have the graph, and we also have information of who spoke to who. So we have our framework!\n","\n","This means we can explore ideas contained in two of the papers you will be reading: . “No country for old members: User lifecycle and linguistic change in online communities.”, and  “Fitting In or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness”, both of which you can access on Canvas.\n","\n","Let us use a simplified version of the papers, and check if a higher number of conversations might lead to a higher similarity between the word distributions for two characters. We can use the same divergences we used in the last notebook. Do you think it will match with the number of times each character was associated with each other?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCXANeVFu4Tx"},"outputs":[],"source":["def kl_divergence(X, Y):\n","    P = X.copy()\n","    Q = Y.copy()\n","    P.columns = ['P']\n","    Q.columns = ['Q']\n","    df = Q.join(P).fillna(0)\n","    p = df.iloc[:,1]\n","    q = df.iloc[:,0]\n","    D_kl = scipy.stats.entropy(p, q)\n","    return D_kl\n","\n","def chi2_divergence(X,Y):\n","    P = X.copy()\n","    Q = Y.copy()\n","    P.columns = ['P']\n","    Q.columns = ['Q']\n","    df = Q.join(P).fillna(0)\n","    p = df.iloc[:,1]\n","    q = df.iloc[:,0]\n","    return scipy.stats.chisquare(p, q).statistic\n","\n","def Divergence(corpus1, corpus2, difference=\"KL\"):\n","    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n","    freqP = nltk.FreqDist(corpus1)\n","    P = pd.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n","    freqQ = nltk.FreqDist(corpus2)\n","    Q = pd.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n","    if difference == \"KL\":\n","        return kl_divergence(P, Q)\n","    elif difference == \"Chi2\":\n","        return chi2_divergence(P, Q)\n","    elif difference == \"KS\":\n","        try:\n","            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n","        except:\n","            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n","    elif difference == \"Wasserstein\":\n","        try:\n","            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n","        except:\n","            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7puMo1jpu4Tx"},"outputs":[],"source":["corpora = []\n","for character in actor_network.nodes():\n","    character_words = []\n","    for sentence in actor_network.nodes[character]['words']:\n","        for word in sentence:\n","            character_words.append(word)\n","    corpora.append(lucem_illud.normalizeTokens(character_words))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lm_UV6F-u4Tx"},"outputs":[],"source":["L = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1I3zO1aru4Tx"},"outputs":[],"source":["for p in corpora:\n","    l = []\n","    for q in corpora:\n","        l.append(Divergence(p,q, difference='KS'))\n","    L.append(l)\n","M = np.array(L)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSxqGKImu4Tx"},"outputs":[],"source":["fig = plt.figure()\n","div = pd.DataFrame(M, columns = list(actor_network.nodes()), index = list(actor_network.nodes()))\n","ax = sns.heatmap(div)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWl023jZu4Tx"},"outputs":[],"source":["# np.corrcoef(M_, M)[0]"]},{"cell_type":"markdown","metadata":{"id":"8Xyq3Xc5u4Tx"},"source":["With our two heatplots, we can attempt to do some rudimentary analysis. We can see from our previous plot that Shawn and Belle talk to each other a lot, so do Hope and Bo, and Nicole and Brady, and Lucas and Sami. Do they also talk *like* each other?\n","\n","Kind of, actually: all four of these pairs have a lower distance between them. Now I don't know anything about this particular soap... are these four pairs related? Are they in a relationship, either married or dating, or are they just really good friends?\n","\n","This lays out the frameworks which you can now use to explore your own networks."]},{"cell_type":"markdown","metadata":{"id":"nKO923c-u4Tx"},"source":["# Interactional influence\n","\n","Before we utilize transformers, let's see how to estimate the influence of one speaker on another in order to estimate a kind of interpersonal influence network based on a recent paper by Fangjian Guo, Charles Blundell, Hanna Wallach, and Katherine Heller entitled [\"The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation\"](https://arxiv.org/pdf/1411.2674.pdf). This relies on a kind of point process called a Hawkes process that estimate the influence of one point on another. Specifically, what they estimate is the degree to which one actor to an interpersonal interaction engaged in \"accomodation\" behaviors relative to the other, generating a directed edge from the one to the other."]},{"cell_type":"markdown","metadata":{"id":"uc7Y9uaAu4Tx"},"source":["### First let's look at the output of their analysis:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gR0IMwU0u4Tx"},"outputs":[],"source":["example_name = '12-angry-men'   #example datasets: \"12-angry-men\" or \"USpresident\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QISN5ARu4Tx"},"outputs":[],"source":["result_path = '../data/Bayesian-echo/results/{}/'.format(example_name)\n","if not os.path.isdir(result_path):\n","    raise ValueError('Invalid example selected, only \"12-angry-men\" or \"USpresident\" are avaliable')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pEw_U9Cu4Tx"},"outputs":[],"source":["df_meta_info = pd.read_table(result_path + 'meta-info.txt',header=None)\n","df_log_prob = pd.read_csv(result_path + \"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n","df_influence = pd.read_csv(result_path + 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n","df_participants = pd.read_csv(result_path + 'cast.txt', delim_whitespace=True)\n","person_id = pd.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n","print()\n","print ('Person : ID')\n","person_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhkiQmLou4Tx"},"outputs":[],"source":["def getDensity(df):\n","    data = df#_log_prob['log.prior']\n","    density = scipy.stats.gaussian_kde(data)\n","    width = np.max(data) - np.min(data)\n","    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)\n","    density.covariance_factor = lambda : .25\n","    density._compute_covariance()\n","    return xs, density(xs)"]},{"cell_type":"markdown","metadata":{"id":"7yHCXTcZu4Ty"},"source":["### Plot MCMC (Markov Monte Carlo) trace and the density of log-likelihoods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tl61ysK9u4Ty","scrolled":false},"outputs":[],"source":["fig = plt.figure(figsize=[12,10])\n","\n","plt.subplot(4,2,1)\n","plt.plot(df_log_prob['log.prior'])\n","plt.xlabel('Iterations')\n","plt.title('Trace of log.prior')\n","\n","plt.subplot(4,2,2)\n","x,y = getDensity(df_log_prob['log.prior'])\n","plt.plot(x,y)\n","plt.xlabel('Iterations')\n","plt.title('Density of log.prior')\n","\n","plt.subplot(4,2,3)\n","plt.plot(df_log_prob['log.likelihood'])\n","plt.title('Trace of log.likelihood')\n","plt.xlabel('Iterations')\n","plt.tight_layout()\n","\n","plt.subplot(4,2,4)\n","x,y = getDensity(df_log_prob['log.likelihood'])\n","plt.plot(x,y)\n","plt.xlabel('Iterations')\n","plt.title('Density of log.likelihood')\n","\n","plt.subplot(4,2,5)\n","plt.plot(df_log_prob['log.likelihood.test.set'])\n","plt.title('Trace of log.likelihood.test.set')\n","plt.xlabel('Iterations')\n","plt.tight_layout()\n","\n","plt.subplot(4,2,6)\n","x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n","plt.plot(x,y)\n","plt.xlabel('Iterations')\n","plt.title('Density of log.likelihood.test.set')\n","\n","plt.subplot(4,2,7)\n","plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n","plt.title('Trace of log.prob')\n","plt.xlabel('Iterations')\n","\n","plt.subplot(4,2,8)\n","x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n","plt.plot(x,y)\n","plt.xlabel('Iterations')\n","plt.title('Density of log.prob')\n","\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QtvCkOatu4Ty"},"source":["### Plot the influence matrix between participants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuBGtsRPu4Ty"},"outputs":[],"source":["A = int(np.sqrt(len(df_influence.columns))) #number of participants\n","id_person = {}\n","for p in person_id:\n","    id_person[person_id[p]]=p"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mKp2go6u4Ty"},"outputs":[],"source":["def getmatrix(stacked,A):\n","    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n","    for row in stacked.iteritems():\n","        from_ = int(row[0].split('.')[1])-1\n","        to_ = int(row[0].split('.')[2])-1\n","        value = float(row[1])\n","        influence_matrix[from_][to_]=value\n","    df_ = pd.DataFrame(influence_matrix)\n","\n","    df_ =df_.rename(index = id_person)\n","    df_ =df_.rename(columns = id_person)\n","    return df_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwXt6Jjau4Ty"},"outputs":[],"source":["stacked = df_influence.mean(axis=0)\n","df_mean = getmatrix(stacked,A)\n","\n","stacked = df_influence.std(axis=0)\n","df_std = getmatrix(stacked,A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZiDBL_Tu4Ty"},"outputs":[],"source":["df_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UxyVIo_u4Ty"},"outputs":[],"source":["f, ax = plt.subplots(figsize=(9, 6))\n","seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n","print('MEAN of influence matrix (row=from, col=to)')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuPFyfa5u4Ty"},"outputs":[],"source":["f, ax = plt.subplots(figsize=(9, 6))\n","seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n","print('SD of influence matrix (row=from, col=to)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HBEDpRYiu4Ty"},"source":["### Barplot of total influences sent/received"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_ncnkFuu4Tz"},"outputs":[],"source":["sender_std = {} #sd of total influence sent\n","reciever_std = {} #sd of total influence recieved\n","for i in range(A):\n","    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n","    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n","\n","sent = df_mean.sum(axis=1) #mean of total influence sent\n","recieved =df_mean.sum(axis=0) #mean of total influence recieved"]},{"cell_type":"markdown","metadata":{"id":"zcwtbGkru4Tz"},"source":["Total influence:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no35DLoeu4Tz"},"outputs":[],"source":["print (\"\\t\\tTotal linguistic influence sent/received \")\n","ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n","\n","plt.grid()\n","wd=0.45\n","ii=0\n","for p in sender_std:\n","    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n","    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n","    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n","    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n","    ii+=1\n","ii=0\n","for p in reciever_std:\n","    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n","    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n","    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n","    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n","    ii+=1\n","plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n","plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n","plt.ylabel('value')\n","plt.xlabel('speaker',fontsize=14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Erlt5NtCu4Tz"},"source":["## Visualize Influence Network!\n","\n","You can visualize any of the influence matrices above:"]},{"cell_type":"markdown","metadata":{"id":"NfvNdgTZu4Tz"},"source":["Using networkx:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdCdG8equ4Tz"},"outputs":[],"source":["def drawNetwork(df,title):\n","    fig = plt.figure(figsize=[8,8])\n","    G = nx.DiGraph()\n","    for from_ in df.index:\n","        for to_ in df.columns:\n","            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n","\n","    pos = nx.spring_layout(G,k=0.55,iterations=20)\n","    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n","    weights = np.array(weights)\n","    #weights = weights*weights\n","    weights = 6*weights/np.max(weights)\n","    print(title)\n","\n","    edge_colors=20*(weights/np.max(weights))\n","    edge_colors = edge_colors.astype(int)\n","#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n","#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n","#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n","    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n","            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n","            alpha=0.75,arrows=True,arrowsize=20)\n","    return edge_colors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZDfo4M0u4Tz"},"outputs":[],"source":["# get quantile influence matrices for 25%, 50%, 75% quantile\n","stacked = df_influence.quantile(0.25)\n","df_q25 = getmatrix(stacked,A)\n","\n","stacked = df_influence.quantile(0.5)\n","df_q50 = getmatrix(stacked,A)\n","\n","stacked = df_influence.quantile(0.75)\n","df_q75 = getmatrix(stacked,A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DepRYxPvu4Tz"},"outputs":[],"source":["G_mean = drawNetwork(df_mean,'Mean Influence Network')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVYazdidu4Tz"},"outputs":[],"source":["G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHTB7ssLu4Tz"},"outputs":[],"source":["G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByG73MOTu4Tz"},"outputs":[],"source":["def fakeEnglish(length):\n","    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n","    return ''.join(np.random.choice(listd,length))"]},{"cell_type":"markdown","metadata":{"id":"iWjnJUsWu4Tz"},"source":["Your own dataset should contains 4 columns (with the same column names) as the artificial one below:\n","\n","- name: name of the participant\n","- tokens: a list of tokens in one utterance\n","- start: starting time of utterance (unit doesn't matter, can be 'seconds','minutes','hours'...)\n","- end: ending time of utterance (same unit as start)\n","\n","There is no need to sort data for the moment.\n","\n","Below, we generate a fake collection of data from \"Obama\", \"Trump\", \"Clinton\"...and other recent presidents. You can either create your own simulation OR (better), add real interactional data from a online chat forum, comment chain, or transcribed from a conversation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gobXyvZMu4Tz"},"outputs":[],"source":["script= []\n","language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n","role = 'Adult' #parameter, no need to tune\n","\n","for i in range(290):\n","    dt = []\n","    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n","    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n","    dt.append(faketokens) #fake utterance\n","    dt.append(i*2+np.random.random()) # start time\n","    dt.append(i*2+1+np.random.random()) # end time\n","    script.append(dt)\n","\n","df_transcript = pd.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n","df_transcript[:2]"]},{"cell_type":"markdown","metadata":{"id":"se3wxvNuu4Tz"},"source":["Transform data into TalkbankXML format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfBLvnEnu4Tz"},"outputs":[],"source":["output_fname = 'USpresident.xml'  #should be .xml\n","language = 'eng'\n","#language = 'chinese'\n","lucem_illud.make_TalkbankXML(df_transcript, output_fname, language = language )"]},{"cell_type":"markdown","metadata":{"id":"25e0Y8F_u4Tz"},"source":["Run Bayesian Echo Chamber to get estimation.\n","\n","- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n","- Larger \"Vocab_size\" (see below) will cost more time\n","- Larger \"sampling_time\" will also consume more time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6omtn8zhu4T0"},"outputs":[],"source":["Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n","sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n","lucem_illud.bec_run(output_fname, Vocab_size, language, sampling_time)"]},{"cell_type":"markdown","metadata":{"id":"6HFEqCj5u4T0"},"source":["## <font color=\"red\">*Exercise 2*</font>\n","\n","<font color=\"red\">Construct cells immediately below this that perform a similar social similarity or influence analysis on a dataset relevant to your final project (__or one from ConvoKit__). Create relationships between actors in a network based on your dataset (e.g., person to person or document to document), and perform analyses that interrogate the structure of their interactions, similarity, and/or influence on one another. (For example, if relevant to your final project, you could explore different soap operas, counting how many times a character may have used the word love in conversation with another character, and identify if characters in love speak like each other. Or do opposites attract?) What does that analysis and its output reveal about the relative influence of each actor on others? What does it reveal about the social game being played?\n","\n","<font color=\"red\">Stretch 1:\n","Render the social network with weights (e.g., based on the number of scenes in which actors appear together), then calculate the most central actors in the show.Realtime output can be viewed in shell.\n","\n","<font color=\"red\">Stretch 2:\n","Implement more complex measures of similarity based on the papers you have read."]},{"cell_type":"markdown","metadata":{"id":"5uKnCDS1u4T0"},"source":["# Text Generation & Converstaions via LangChain\n","\n","We can make use of the LLMs we learned in week 6 to do text generation, where the model takes one or multiple places in a conversation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n","\n","These models can be quite impressive, even uncanny in how human like they sound. We mainly use the package LangChain to interact with LLMs."]},{"cell_type":"markdown","metadata":{"id":"nTDgRy0jKDkP"},"source":["## Overview:\n","- Installation and setup (old packages!)\n","- LLMs (Open AI and Open Source)\n","- Prompt Templates and Chains (Smooth communication between models)\n","- Agents and Tools, Memory, Documents\n","- Simulations and Conversations\n","- Simularca"]},{"cell_type":"markdown","metadata":{"id":"5WGtOYYTKfz3"},"source":["## Installation\n","\n","In the rapidly evolving landscape of LLMs, packages and libraries like LangChain are frequently updated to leverage the latest advancements. We've chosen this particular version of LangChain for its ease of use in a Colab notebook, effectively demonstrating the power of LLMs as simulators. However, it's important to note that this code is a year old. If you're planning to use it in a production environment, it will be essential to update and adapt it to align with the latest versions and best practices in the field, ensuring compatibility and optimal performance."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16927,"status":"ok","timestamp":1708286421991,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"bcrn7QRyQXGj","outputId":"3466932e-87db-4e9d-f5bc-1f1118376d81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain==0.0.316\n","  Downloading langchain-0.0.316-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.0.27)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.9.3)\n","Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.7.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.316)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.316)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langsmith<0.1.0,>=0.0.43 (from langchain==0.0.316)\n","  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.6.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.9.4)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.2.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n","  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.316)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (2.16.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.316) (3.0.3)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316) (23.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain\n","Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.316 langsmith-0.0.92 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain==0.0.316"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12247,"status":"ok","timestamp":1708286434218,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"var0F2Glwnv0","outputId":"fe3a7c3f-9388-4b3d-9677-d2e1955da1a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai==0.28.1\n","  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n","Installing collected packages: openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed openai-0.28.1\n"]}],"source":["!pip install openai==0.28.1"]},{"cell_type":"markdown","metadata":{"id":"NkGGSdmtta6s"},"source":["## LLMs\n","\n","A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"]},{"cell_type":"markdown","metadata":{"id":"6OFLmmrtejCl"},"source":["### Open AI via LangChain"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1708286459911,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"RlxEmS1CaM5v"},"outputs":[],"source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1660,"status":"ok","timestamp":1708286464471,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"pY09s9cmZ6nQ","outputId":"f51e2a8c-68f0-445a-bd84-316eb138f59c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\"Rainbow Feet Co.\" or \"Vibrant Soles Inc.\"\n"]}],"source":["from langchain.llms import OpenAI\n","\n","llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\")\n","text = \"What would be a good company name for a company that makes colorful socks?\"\n","print(llm_openai(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YNn2fvVY1iX"},"outputs":[],"source":["# if you wish, you can also choose to use GPT-4, which is better. But the way to construct message is a bit different\n","# https://python.langchain.com/docs/integrations/chat/openai\n","\n","from langchain.chat_models import ChatOpenAI\n","llm = ChatOpenAI(temperature=0.9, model_name=\"gpt-4-turbo\")"]},{"cell_type":"markdown","metadata":{"id":"A-z3EauZena2"},"source":["### Using other open-source LLMs"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6126,"status":"ok","timestamp":1708286480429,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"idkq_aVyaceF","outputId":"686cc20e-2a8a-4fac-df43-efd28a5d685d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"]}],"source":["!pip install huggingface_hub"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":174,"status":"ok","timestamp":1708286486132,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"i4DKOWjyaRmO"},"outputs":[],"source":["import os\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1708286490008,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"QmtH72oCaU32"},"outputs":[],"source":["from langchain import HuggingFaceHub"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4561,"status":"ok","timestamp":1708286495137,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"8uK5TtJPc49I","outputId":"e0b4e96c-5f09-44dd-9664-7c77fba6275e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":[" Our current thought is ColourSock. Also, if you have any tips for branding, that would be helpful as well!\n","\n","A name that describes exactly what the products are is fine, but you should probably have another name that is your \"brand name.\" The brand name can emphasize the colorful and playful aspects, while not necessarily having to tie it in with the type of product.\n","For example, you could create a name like \"HappySocks\" or\n"]}],"source":["# https://huggingface.co/google/flan-t5-xl\n","llm_mistral = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n","\n","print(llm_mistral(prompt = \"What would be a good company name for a company that makes colorful socks?\"))"]},{"cell_type":"markdown","metadata":{"id":"G7kSceuAq1Bn"},"source":["## Personalities and Options\n","\n","In the `get_meal_options` function, the integration of history and context prompts plays a crucial role in shaping the AI's conversation. The `personality_template`, which serves as a context prompt, establishes the AI's expertise in vegetarian, Italian cuisine, and healthy eating. This template sets the stage for the conversation, informing the AI about its role and area of specialization. It's akin to giving the AI a 'character' or 'personality' to adhere to during the interaction, ensuring that its responses are aligned with this predefined context.\n","\n","The history prompts, represented by `example_human_history` and `example_ai_history`, simulate a prior interaction between the human and the AI, adding depth to the conversation's backstory. These prompts are crucial for creating a more natural and realistic interaction, as they give the impression that the conversation is ongoing rather than starting from scratch. When combined with the context prompt in `ChatPromptTemplate`, they create a comprehensive conversation framework. This framework effectively guides the AI's responses to be consistent with both the established personality (context) and the implied history of the interaction, leading to more coherent and relevant meal suggestions for breakfast, lunch, and dinner."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1913,"status":"ok","timestamp":1708286503058,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"wyMylKqhu-IL"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import LLMChain\n","\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    AIMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1708286503068,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"hDIMf8akfE9a"},"outputs":[],"source":["def get_meal_options(personality_template, human_history=\"\", ai_history=\"\"):\n","    # Initialize ChatOpenAI with OpenAI model\n","    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # Replace with your API key\n","\n","\n","    example_human_history = HumanMessagePromptTemplate.from_template(human_history)\n","    example_ai_history = AIMessagePromptTemplate.from_template(ai_history)\n","\n","    system_message_prompt = SystemMessagePromptTemplate.from_template(personality_template)\n","\n","    human_template=\"{input}\"\n","    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","\n","    # Create ChatPromptTemplate\n","    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human_history, example_ai_history, human_message_prompt])\n","    # Initialize the conversation chain\n","    chain = LLMChain(llm=chat, prompt=chat_prompt)\n","\n","    # Ask for meal options\n","    meals = ['breakfast', 'lunch', 'dinner']\n","    meal_options = {}\n","    for meal in meals:\n","        response = chain.run(f\"What are some good {meal} options for someone who loves vegetarian, Italian cuisine and prefers healthy options?\")\n","        meal_options[meal] = response\n","\n","    return meal_options\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1708286503278,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"vywW6mdswKSA"},"outputs":[],"source":["# Define the personality based on food preferences\n","personality_template = \"\"\"\n","The following is a conversation with an AI who is an expert in vegetarian, Italian cuisine and healthy eating options.\n","AI: I am here to provide creative and healthy vegetarian meal suggestions. Ask me anything about vegetarian food!\n","\"\"\"\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12246,"status":"ok","timestamp":1708286515818,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"i2z822LLv6iY","outputId":"1c947564-4e42-47bb-f254-c23f1faf5508"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'breakfast': 'For a delicious and healthy Italian-inspired vegetarian breakfast, you can try the following options:\\n\\n1. **Caprese Breakfast Sandwich**: Use whole grain bread, sliced tomatoes, fresh mozzarella, basil leaves, and a drizzle of balsamic glaze for a tasty and filling breakfast sandwich.\\n\\n2. **Vegetarian Frittata**: Make a frittata using eggs or tofu as a base and add in your favorite Italian vegetables like tomatoes, bell peppers, spinach, and mushrooms. Top with some grated Parmesan cheese for extra flavor.\\n\\n3. **Bruschetta Avocado Toast**: Top whole grain toast with mashed avocado, cherry tomatoes, basil, and a sprinkle of salt and pepper. Drizzle with balsamic glaze for a flavorful twist.\\n\\n4. **Italian Style Smoothie**: Blend together frozen mixed berries, spinach, banana, almond milk, and a scoop of protein powder for a nutritious and refreshing breakfast option.\\n\\n5. **Ricotta Pancakes**: Make fluffy pancakes using ricotta cheese in the batter. Serve with fresh berries and a dollop of Greek yogurt for a protein-packed breakfast.\\n\\nThese options are not only delicious but also provide a good balance of nutrients to start your day on a healthy note. Enjoy your Italian-inspired vegetarian breakfast!', 'lunch': 'One delicious and healthy Italian vegetarian lunch option is a Caprese salad with a twist. Instead of just tomatoes, mozzarella, and basil, you can add some grilled vegetables like zucchini, bell peppers, and eggplant to make it more satisfying and nutritious. Drizzle with balsamic glaze and olive oil for extra flavor.\\n\\nAnother great option is a whole wheat vegetable lasagna. Layer whole wheat lasagna noodles with your favorite sautéed vegetables like spinach, mushrooms, and bell peppers, along with ricotta and marinara sauce. Top with a sprinkle of mozzarella and bake until bubbly and golden.\\n\\nFor a quick and easy option, you can make a vegetarian panini with whole grain bread, grilled vegetables, pesto, and a slice of fresh mozzarella. Serve with a side of mixed greens dressed with balsamic vinaigrette for a complete and satisfying meal.', 'dinner': \"There are many delicious and healthy vegetarian Italian dinner options to choose from! Here are some suggestions:\\n\\n1. Eggplant Parmesan: A classic Italian dish made with baked or grilled eggplant slices layered with marinara sauce, mozzarella, and Parmesan cheese. You can also use whole wheat breadcrumbs for added fiber.\\n\\n2. Caprese Salad: A simple and refreshing salad made with fresh tomatoes, basil, and mozzarella cheese, drizzled with balsamic glaze and olive oil. It's a light and flavorful option for dinner.\\n\\n3. Lentil Bolognese: A hearty and protein-rich alternative to traditional meat-based Bolognese sauce, made with cooked lentils, tomatoes, onions, and garlic. Serve over whole wheat pasta for a nutritious meal.\\n\\n4. Stuffed Bell Peppers: Fill bell peppers with a mixture of quinoa, black beans, corn, tomatoes, and spices, then bake until tender. This dish is packed with fiber, protein, and vitamins.\\n\\n5. Zucchini Noodles with Pesto: Replace traditional pasta with zucchini noodles (zoodles) and toss them with homemade pesto sauce made with basil, pine nuts, garlic, and olive oil. It's a light and low-carb option.\\n\\n6. Risotto Primavera: A creamy and comforting risotto dish made with Arborio rice, vegetable broth, seasonal vegetables like peas, asparagus, and carrots, and finished with Parmesan cheese. It's a satisfying and flavorful meal.\\n\\nThese are just a few ideas to get you started. Feel free to customize these recipes to suit your preferences and dietary needs!\"}\n"]}],"source":["# Call the function to get meal options\n","meal_options = get_meal_options(personality_template)\n","print(meal_options)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3O-7dO1htdO4"},"source":["## Prompt Templates and Chaining\n","\n","LangChain faciliates prompt management and optimization.\n","\n","Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.\n","\n","Prompt templates in LangChain are pre-defined formats or structures for creating prompts that are sent to a language model. These templates help in standardizing and automating the way you interact with the model, ensuring consistency and potentially improving the quality of responses. They are particularly useful when you need to generate prompts that follow a specific format or include certain fixed elements.\n","\n","For example, you might have a prompt template for a customer service bot where each prompt starts with a greeting, includes the user's question, and ends with a closing statement. By using a template, you can easily insert the user's specific question into the prompt without having to manually format the entire prompt each time.\n","\n","This can also be useful if you are conducting a research experiment to consistently measure responses, or for annotation purposes."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":97,"status":"ok","timestamp":1708286519050,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"_FDS9IDRapOt"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":131,"status":"ok","timestamp":1708286519493,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"LtIaMPKVx_Q7"},"outputs":[],"source":["# Define the template for sarcasm annotation\n","sarcasm_template = \"\"\"\n","Analyze the following text to determine if it is sarcastic or not.\n","\n","Text: \"{text}\"\n","\n","Is the text sarcastic? Provide a brief explanation for your decision:\n","\"\"\""]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":106,"status":"ok","timestamp":1708286519773,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"hBOCff6ryBGe"},"outputs":[],"source":["# Create a PromptTemplate instance with the defined template\n","sarcasm_prompt = PromptTemplate(template=sarcasm_template, input_variables=[\"text\"])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1708286520210,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"DO77wA568zLv","outputId":"29528112-0688-4452-aa54-eaf642075bfe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PromptTemplate(input_variables=['text'], template='\\nAnalyze the following text to determine if it is sarcastic or not.\\n\\nText: \"{text}\"\\n\\nIs the text sarcastic? Provide a brief explanation for your decision:\\n')"]},"metadata":{},"execution_count":16}],"source":["sarcasm_prompt"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":105,"status":"ok","timestamp":1708286521069,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"-Yzpc_0aHHeE"},"outputs":[],"source":["# Example text to be analyzed\n","example_text = \"Oh great, another Monday morning!\"\n","\n","# Format the prompt with the specific text\n","prompt_text = sarcasm_prompt.format(text=example_text)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1708286521627,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"on8ubh3kt7oD","outputId":"e150daed-fb78-4338-c6b2-49028a6a923b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nAnalyze the following text to determine if it is sarcastic or not.\\n\\nText: \"Oh great, another Monday morning!\"\\n\\nIs the text sarcastic? Provide a brief explanation for your decision:\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["prompt_text"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1043,"status":"ok","timestamp":1708286524001,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"BwtCx_aAghQi","outputId":"fbb52169-d19a-4e60-9b4d-a9565317731a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Yes, the text is likely to be sarcastic. The use of \"Oh great\" before mentioning Monday morning suggests a negative connotation towards the start of the work week. This tone is often associated with sarcasm, as the speaker is expressing the opposite of what they actually mean.\n"]}],"source":["print(llm_openai(prompt_text))"]},{"cell_type":"markdown","metadata":{"id":"1zw1KlSeuUOY"},"source":["## Chains\n","\n","Chaining refers to the process of using the output from one model as the input to another model, or even as input back into the same model, in a sequential manner. This allows for more complex interactions and workflows that go beyond single-turn question-answer formats.\n","\n","In the context of LangChain, chaining can be used to create multi-step processes or workflows. For instance, you might first use a language model to generate a draft response to a user's query, then pass this draft through another model (or the same model) for further refinement, summarization, or translation.\n","\n","Chaining can be powerful in scenarios where a single interaction with a model is insufficient to achieve the desired outcome. It enables more sophisticated applications, such as:\n","\n","Iterative Refinement: Where the output of a model is refined over several iterations to improve quality or detail.\n","Multi-Model Workflows: Where different models are used for different tasks in a sequence, like one model generating content and another summarizing it.\n","Feedback Loops: Where the output of a model is fed back as input to the same model, possibly with modifications, to simulate a conversation or to gradually steer the model towards a specific goal.\n","In LangChain, chaining is facilitated by its architecture, which allows for easy integration and sequencing of different models and processes.\n","\n","Below is the simplest example of such a chain -> simply passing a prompt and model and then \"running\" will pass the message to the model."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":923,"status":"ok","timestamp":1708286527926,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"eE6n-jbAuOxt","outputId":"d1444b66-aeda-4c7e-a51f-1dc7e32eec74"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Yes, the text is sarcastic. The speaker is using an exaggerated and sarcastic tone to express their dissatisfaction with the idea of working hard for very little pay and the idea that class distinctions are a positive aspect of society.\n"]}],"source":["from langchain import LLMChain\n","\n","llm_chain = LLMChain(prompt=sarcasm_prompt, llm=llm_openai)\n","\n","question = \"Oh, awesome, I love working hard and making minimum wage. Class distinctions are a good thing.\"\n","\n","print(llm_chain.run(question))"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1708286527926,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"YgRYGSrTEGDZ"},"outputs":[],"source":["text_gen_template = \"Write a creative, humorous and sarcastic statement about {text}.\""]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1708286527926,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"3y2p6rqsHJ7l"},"outputs":[],"source":["prompt_gen = PromptTemplate(template=text_gen_template, input_variables=[\"text\"])"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1708286527926,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"4GvOyi8pEGGI"},"outputs":[],"source":["llm_chain_gen = LLMChain(prompt=prompt_gen, llm=llm_mistral)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708286528066,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"_kgZmY4vEGI9"},"outputs":[],"source":["sarcastic_subject = \"Mondays\""]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":4371,"status":"ok","timestamp":1708286532774,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"BNa_umQ5HmD3","outputId":"0302b03b-0f55-4da1-a301-712d5655c49e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\nUgh... It's Monday again! That delightful day where dreams of weekend freedom shatter as reality comes crashing down. The perfect time to relish in the joy of alarms, traffic jams, and the ever-growing mountain of emails from people who apparently have nothing better to do on the weekends than to think up new ways to bother you. Mondays: When the sweet taste of freedom morphs into the bitter tang of responsibility, and Saturday's\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}],"source":["llm_chain_gen.run(sarcastic_subject)"]},{"cell_type":"markdown","metadata":{"id":"Zp-UlOK0bMVQ"},"source":["## Agents and Tools\n","\n","Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n","\n","\n","When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n","\n","- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n","- LLM: The language model powering the agent.\n","- Agent: The agent to use.\n","\n","Tools: https://python.langchain.com/en/latest/modules/agents/tools.html\n","\n","Agent Types: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79JcjhFXwv0J"},"outputs":[],"source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5288,"status":"ok","timestamp":1708221978456,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"dOSpaurEb1MR","outputId":"c849cdc1-d73f-4b2d-d970-a5a80222cb9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wikipedia\n","  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=5441d70d5409dca9000f9daea485ebc09c488d801664f104a6b70f2cbb6ca4bc\n","  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"]}],"source":["!pip install wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgV4kny1bgy1"},"outputs":[],"source":["from langchain.llms import OpenAI\n","llm = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\")\n","tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQUOsWLrbjKv"},"outputs":[],"source":["agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":4906,"status":"ok","timestamp":1708221983470,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"M8Rob2Wsb_l9","outputId":"f5010b7e-dbaf-4b0e-fb08-5b12afbff2d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m We can use Wikipedia to find the year the film was released and a Calculator to raise it to a power.\n","Action: Wikipedia\n","Action Input: The Departed\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n","Summary: The Departed is a 2006 American crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n","The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n","The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n","It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It became Scorsese's first and, to date, only personal Oscar win; Wahlberg was also nominated for Best Supporting Actor. The film also received six nominations at the 64th Golden Globe Awards, six nominations at the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards. DiCaprio was nominated for Golden Globe Award for Best Actor – Motion Picture Drama (also nominated that year in the same category for Blood Diamond), BAFTA Award for Best Actor in a Leading Role and Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Supporting Role for his performance.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m The film was released in 2006 and we can use a calculator to raise it to the power of 0.43.\n","Action: Calculator\n","Action Input: 2006 ^ 0.43\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n","Final Answer: 26.30281917656938\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'26.30281917656938'"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"]},{"cell_type":"markdown","metadata":{"id":"prqby8Wb9wlx"},"source":["## <font color=\"red\">*Exercise 3*</font>\n","\n","<font color=\"red\">Review the documentation for tools and agents from LangChain . Utilize at least two tools with appropriate agents discovered during your review to construct a chain addressing questions pertinent to your final project. If your project dataset is unsuitable for this task, select an alternative small-sized dataset for implementation."]},{"cell_type":"markdown","metadata":{"id":"9wMttXM-CuPK"},"source":["## Memory\n","\n","We can use the history and system prompts to create a memory (of sorts) for the model."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1708286552532,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"v5Rk-GIXUwdB"},"outputs":[],"source":["from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate\n","from langchain.llms import OpenAI\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1708286552921,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"Pavb7kYhPQnq"},"outputs":[],"source":["# Initialize the language model\n","def create_system_message_prompt():\n","    \"\"\" Creates a system message prompt\"\"\"\n","    personality_template = f\"\"\"\n","    The following is a conversation with an AI assistant.\n","    \"\"\"\n","    return SystemMessagePromptTemplate.from_template(personality_template)\n","\n","def create_chat_prompt(human_history, ai_history):\n","    \"\"\" Creates a chat prompt template with human history, and AI history. \"\"\"\n","    messages = []\n","    system_message_prompt = create_system_message_prompt()\n","\n","    for h, a in zip(human_history, ai_history):\n","      messages.append(HumanMessagePromptTemplate.from_template(h))\n","      messages.append(AIMessagePromptTemplate.from_template(a))\n","\n","    messages.append(HumanMessagePromptTemplate.from_template(\"{input}\"))\n","    return ChatPromptTemplate.from_messages(messages)\n","\n","def query_chain(chain, input_text):\n","    \"\"\" Queries the conversation chain with the given input. \"\"\"\n","    return chain.run(input_text)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1708286553992,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"LRIUPj5aUx35","outputId":"93d79e89-583f-4ca6-b176-f1e8d15bfcd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","AI: Noted. Your favorite color is blue.\n"]}],"source":["# Example usage\n","human_history = []\n","ai_history = []\n","\n","chat_prompt = create_chat_prompt(human_history, ai_history)\n","\n","# Initialize the conversation chain\n","chain = LLMChain(llm=llm_openai, prompt=chat_prompt)\n","\n","# Query the chain\n","human_input =  \"Please take a note that my favorite color is blue.\"\n","ai_response = query_chain(chain, human_input)\n","print(ai_response)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":552,"status":"ok","timestamp":1708286555933,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"NKidm0CyOdJS","outputId":"42077cf9-f483-4ec4-8299-5c42dee42942"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","AI: Your favorite color is blue. \n"]}],"source":["human_history.append(human_input)\n","ai_history.append(ai_response)\n","\n","chat_prompt = create_chat_prompt(human_history, ai_history)\n","\n","# Initialize the conversation chain\n","chain = LLMChain(llm=llm_openai, prompt=chat_prompt)\n","\n","# Query the chain\n","human_input =  \"Could you please remind me what is my favorite color?\"\n","ai_response = query_chain(chain, human_input)\n","print(ai_response)"]},{"cell_type":"markdown","metadata":{"id":"Xpd1gMPnnZnT"},"source":["## Simulation and Conversations\n","\n","Above we showcased a toy LLM conversation between Human and AI with memories. We can go beyond the notion of conversation between AI and Human. We can make both participants AIs who think they are conversing with human! In other words, for example, we can make AI no.1 plays the role Trump and AI no.2 plays the role of Biden, and both of them think they're talking to the real Trump/Biden!\n","\n","Let's implement this idea by using questions from the 2020 predidential election debate as templates. Differnent from the hard coding way of implementing memory, below we use the ConversationBufferMemory class from langchain to implement."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":423,"status":"ok","timestamp":1708286562837,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"juu-QlSfsLxG"},"outputs":[],"source":["from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1708286563202,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"v7A6h7mkbyBv"},"outputs":[],"source":["from langchain.llms import OpenAI\n","\n","llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\", max_tokens=512)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708286563311,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"E4t1qLnw_koX"},"outputs":[],"source":["Trump_memory = ConversationBufferMemory(human_prefix='Trump', ai_prefix='Biden')\n","Biden_memory = ConversationBufferMemory(human_prefix='Biden', ai_prefix='Trump')\n","\n","Trump_template = \"\"\"Imagine you're the Democrats presidential candidate Joe Biden. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Donald Trump. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n","\n","Current conversation:\n","{history}\n","Trump: {input}\n","Biden:\"\"\"\n","Biden_template = \"\"\"\"Imagine you're the Republican presidential candidate Donald Trump. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Joe Biden. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n","\n","Current conversation:\n","{history}\n","Biden: {input}\n","Trump:\"\"\"\n","\n","# By default, models recognize themselves as AIs. So we need to let them do role play\n","# and convince them that they're talking to real Trump/Biden (though they're not)\n","\n","TRUMP_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Trump_template)\n","BIDEN_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Biden_template)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":90,"status":"ok","timestamp":1708286564086,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"uhvM6InzsL3r"},"outputs":[],"source":["Trump_conversation = ConversationChain(\n","    prompt=TRUMP_PROMPT,\n","    llm=llm_openai,\n","    verbose=False,\n","    memory=Trump_memory\n",")"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":3861,"status":"ok","timestamp":1708286568515,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"KQZ2xLvaJBKE"},"outputs":[],"source":["# suppose Biden starts first\n","biden_response = Trump_conversation.predict(input='')"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1708286568526,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"1pXpwpSNa_Yk","outputId":"1df4a5fc-0c43-4ad4-ee0b-ab5b2ef2169b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\"Well, first of all, I want to thank everyone for tuning in tonight. This is an important discussion about the future of our country and the Supreme Court is a crucial part of that. Now, the question at hand is about selecting a Supreme Court nominee in an election year and let me be clear, my position is not only the correct one, it\\'s the only one that honors the American people and our constitution.\\n\\nLet\\'s start with the facts. In 2016, President Obama nominated Merrick Garland to fill the vacant Supreme Court seat. But the Republican-controlled Senate refused to even hold a hearing, let alone a vote. And now, just four years later, these same Republicans are rushing to fill the seat left by the late Justice Ruth Bader Ginsburg. This blatant hypocrisy and disregard for the will of the American people is unacceptable.\\n\\nThe American people are already voting. Millions of ballots have been cast and millions more will be cast before Election Day. In fact, in 2016, Senate Majority Leader McConnell argued that the American people should have a say in the selection of the next Supreme Court Justice and that the nomination should wait until after the election. I couldn\\'t agree more. The American people deserve to have a voice in this process and that voice is their vote.\\n\\nThis is not about Democrats or Republicans, this is about the integrity of our democracy. Rushing a nominee through in the midst of an election is a dangerous precedent and it undermines the legitimacy of the Supreme Court. The American people deserve a Supreme Court that is fair and impartial, not one that is stacked with partisan justices.\\n\\nSo, my position is clear. As President, I would honor the will of the American people and wait until after the election to nominate a Supreme Court Justice. This is the only way to ensure that the American people have a say in the future of the court and that our democracy remains strong. Thank you.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}],"source":["biden_response"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1708286568526,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"1kFjxUnDI1Fp"},"outputs":[],"source":["Biden_conversation = ConversationChain(\n","    prompt=BIDEN_PROMPT,\n","    llm=llm_openai,\n","    verbose=False,\n","    memory=Biden_memory\n",")"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":3747,"status":"ok","timestamp":1708286572248,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"emFitHWQXl1T"},"outputs":[],"source":["trump_response = Biden_conversation.predict(input=biden_response)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1708286572248,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"e-9ZcknccNku","outputId":"f8cc81b3-bc80-4d51-e7dc-b76027be44bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["' \\n\"Thank you, Joe, for your answer. But let\\'s be real here, the American people did have a say in the selection of the next Supreme Court Justice when they elected me as their President in 2016. And just like any other President, I have the constitutional duty to nominate a Supreme Court Justice when there is a vacancy. And that\\'s exactly what I am doing.\\n\\nRushing a nominee through? Joe, I don\\'t know if you\\'ve been paying attention, but we have plenty of time before the election to nominate and confirm a justice. And that\\'s exactly what we are going to do – follow the Constitution and do our job.\\n\\nNow, let\\'s talk about the hypocrisy you mentioned. The difference between 2016 and now is that in 2016, we had a divided government – a Democratic President and a Republican-controlled Senate. That is not the case now. We have a Republican President and a Republican-controlled Senate. And that\\'s why we can move forward with a nomination and confirmation.\\n\\nBut let\\'s not forget, Joe, that you were the Vice President during the 2016 election and you also voted to confirm a Supreme Court Justice during an election year. So, if anyone is being hypocritical here, it\\'s you.\\n\\nFurthermore, the American people elected me to fulfill a duty and make decisions for the country. And that\\'s exactly what I am doing by selecting a highly qualified nominee for the Supreme Court. I will not let political games and partisan politics get in the way of fulfilling my responsibilities as President.\\n\\nIn the end, this is not about Democrats or Republicans, this is about fulfilling my constitutional duty and putting a qualified individual on the Supreme Court. And that\\'s exactly what I will do. Thank you.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["trump_response"]},{"cell_type":"markdown","metadata":{"id":"ot9bFycQcfC9"},"source":["Well, seems that too nice to be Trump. Anyway, now we can make the debate run by feeding Trump agent's and Biden agent's response to each other."]},{"cell_type":"markdown","metadata":{"id":"nlITuIPVncVi"},"source":["## Simularca\n","\n","One of the most influential generative LLM works in 2023 is the [Generative Agents: Interactive Simulacra of Human Behavior](https://dl.acm.org/doi/abs/10.1145/3586183.3606763) from Stanford. In this paper, the authors constructed a small town with 25 agents driven by ChatGPT:\n","> Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents—computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architec- ture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behav- iors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.\n","\n","<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/1.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/1.png?raw=true\" style=\"width:500px\">\n","\n","### Agent System\n","\n","In the paper, the agents have memory and can reflect. Agents can to remember, retrieve, reflect, interact with other agents, and plan through dynamically evolving circumstances.\n","\n","<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/2.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/2.png?raw=true\" style=\"width:500px\">\n","\n","They open-sourced the code on [GitHub](https://github.com/joonspk-research/generative_agents)."]},{"cell_type":"markdown","metadata":{"id":"JoBKxJNs_8Gh"},"source":["### Memory and Information Retrieval\n","\n","Memory and Information Retrieval is the key to set up the whole town, along with ChatGPT, making agents differnent from those simple rule-based agents.\n","\n",">Approach: The memory stream maintains a comprehensive record of the agent’s experience. It is a list of memory objects, where each object contains a natural language description, a creation times- tamp and a most recent access timestamp. The most basic element of the memory stream is an observation, which is an event directly perceived by an agent. Common observations include behaviors performed by the agent themselves, or behaviors that agents per- ceive being performed by other agents or non-agent objects. For instance, Isabella Rodriguez, who works at a coffee shop, might ac- crue the following observations over time: (1) Isabella Rodriguez is setting out the pastries, (2) Maria Lopez is studying for a Chemistry test while drinking coffee, (3) Isabella Rodriguez and Maria Lopez are conversing about planning a Valentine’s day party at Hobbs Cafe, (4) The refrigerator is empty.\n","\n",">Our architecture implements a retrieval function that takes the agent’s current situation as input and returns a subset of the mem- ory stream to pass on to the language model. There are many pos- sible implementations of a retrieval function, depending on what it is important that the agent consider when deciding how to act. In our context, we focus on three main components that together produce effective results.\n","\n","> *Recency* assigns a higher score to memory objects that were re- cently accessed, so that events from a moment ago or this morning are likely to remain in the agent’s attentional sphere. In our im- plementation, we treat recency as an exponential decay function over the number of sandbox game hours since the memory was last retrieved. Our decay factor is 0.99.\n","\n","> *Importance* distinguishes mundane from core memories, by as- signing a higher score to those memory objects that the agent believes to be important. For instance, a mundane event such as eating breakfast in one’s room would yield a low importance score, whereas a breakup with one’s significant other would yield a high score. There are again many possible implementations of an im- portance score; we find that directly asking the language model to output an integer score is effective.\n","\n","> *Relevance* assigns a higher score to memory objects that are related to the current situation. What is relevant depends on the answer to, “Relevant to what?”, so we condition relevance on a query memory. If the query, for example, is that a student is dis- cussing what to study for a chemistry test with a classmate, memory objects about their breakfast should have low relevance, whereas memory objects about the teacher and schoolwork should have high relevance. In our implementation, we use the language model to generate an embedding vector of the text description of each memory. Then, we calculate relevance as the cosine similarity be- tween the memory’s embedding vector and the query memory’s embedding vector.\n","\n","> To calculate the final retrieval score, we normalize the recency, relevance, and importance scores to the range of [0, 1] by min-max scaling. The retrieval function scores all memories as a weighted combination of the three elements:$ score = \\alpha_{recency} * recency + \\alpha_{importance} * importance + \\alpha_{relevance} * relevance$ In our implementation, all $\\alpha$'s are set to 1. The top-ranked memories that fit in the language model’s context window are then included in the prompt."]},{"cell_type":"markdown","metadata":{"id":"mGIKO6j6ABH1"},"source":["<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/3.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks-2024-Winter/blob/main/data/Simularca/3.png?raw=true\" style=\"width:500px\">"]},{"cell_type":"markdown","metadata":{"id":"6nfNkMjnKeUk"},"source":["To give you a sense of how the memory retrieval algorithm works, I will use Trump's tweets data to select several tweets that most pertinent to the presidential debate question to prompt the Trump agent respond to the Biden agent in a different way. In this case, we assume the Trump agent learns how Trump behaves on Twitter and this may help the Trump agent prepare for the presidential debate (Ideally, we should provide Trump's previous television debate and also provide similar materials for the Biden agent to learn. I don't have Biden's tweets at hand, you're welcome to create a tweet-based version Biden agent for the debate!)"]},{"cell_type":"markdown","metadata":{"id":"uPI1xqTgu4T0"},"source":["### Memory Retrieval Exmaple\n","\n"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab\\ Notebooks/Computational\\ Content\\ Analysis/Homework-Notebooks-2024-Winter/week\\ 8"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Si7TARe80XS5","executionInfo":{"status":"ok","timestamp":1708286716661,"user_tz":360,"elapsed":100,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"0ab54fd8-cc49-4ad6-e3b8-f978ca727802"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Computational Content Analysis/Homework-Notebooks-2024-Winter/week 8\n"]}]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":915,"status":"ok","timestamp":1708286718968,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"PtjSPKGAhdpH"},"outputs":[],"source":["import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1708286719953,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"AroBz4dxu4T0"},"outputs":[],"source":["dfs = []"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":6053,"status":"ok","timestamp":1708286726320,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"ueMbqHgXu4T0"},"outputs":[],"source":["for file in os.listdir(\"../data/trump_tweets\"):\n","    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":166,"status":"ok","timestamp":1708286896111,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"IjuFjnACu4T0"},"outputs":[],"source":["df = pd.concat(dfs)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1708286896441,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"N6tcRsdNu4T0","outputId":"c41cb68e-6d83-4355-e886-7322c7f18fe3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    source              id_str  \\\n","0      Twitter for Android  550441250965708800   \n","1      Twitter for Android  550441111513493504   \n","2      Twitter for Android  550440752254562304   \n","3      Twitter for Android  550440620792492032   \n","4      Twitter for Android  550440523094577152   \n","...                    ...                 ...   \n","23655   Twitter for iPhone  950946293077209088   \n","23656   Twitter for iPhone  950937350003183616   \n","23657   Twitter for iPhone  950884128379035648   \n","23658   Twitter for iPhone  950879772644659200   \n","23659   Twitter for iPhone  950866561153331200   \n","\n","                                                    text  \\\n","0      \"@ronmeier123: @Macys Your APPAREL is UNPARALL...   \n","1      \"@gillule4: @realDonaldTrump incredible experi...   \n","2      \"@JobSnarechs: Negotiation tip #1: The worst t...   \n","3      \"@joelmch2os: @realDonaldTrump announce your p...   \n","4      \"@djspookyshadow: Feeling a deep gratitude for...   \n","...                                                  ...   \n","23655  .@ICEgov HSI agents and ERO officers, on behal...   \n","23656  Thank you @GOPLeader Kevin McCarthy! Couldn’t ...   \n","23657  As I made very clear today, our country needs ...   \n","23658  RT @Scavino45: “Utilities cutting rates, cite ...   \n","23659  Today, it was my great honor to sign a new Exe...   \n","\n","                     created_at  retweet_count  in_reply_to_user_id_str  \\\n","0     2014-12-31 23:59:55+00:00              8                      NaN   \n","1     2014-12-31 23:59:22+00:00              5                      NaN   \n","2     2014-12-31 23:57:56+00:00             33                      NaN   \n","3     2014-12-31 23:57:25+00:00              8                      NaN   \n","4     2014-12-31 23:57:02+00:00              9                      NaN   \n","...                         ...            ...                      ...   \n","23655 2018-01-10 04:23:58+00:00          13822                      NaN   \n","23656 2018-01-10 03:48:26+00:00          11893                      NaN   \n","23657 2018-01-10 00:16:57+00:00          24552                      NaN   \n","23658 2018-01-09 23:59:38+00:00          11716                      NaN   \n","23659 2018-01-09 23:07:09+00:00          19481                      NaN   \n","\n","       favorite_count  is_retweet  \n","0                  21       False  \n","1                  18       False  \n","2                  44       False  \n","3                  26       False  \n","4                  31       False  \n","...               ...         ...  \n","23655           60252       False  \n","23656           51951       False  \n","23657          123413       False  \n","23658               0        True  \n","23659           82151       False  \n","\n","[23660 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-fb3178fe-70ab-44ff-82f0-b120bc652e58\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>id_str</th>\n","      <th>text</th>\n","      <th>created_at</th>\n","      <th>retweet_count</th>\n","      <th>in_reply_to_user_id_str</th>\n","      <th>favorite_count</th>\n","      <th>is_retweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Twitter for Android</td>\n","      <td>550441250965708800</td>\n","      <td>\"@ronmeier123: @Macys Your APPAREL is UNPARALL...</td>\n","      <td>2014-12-31 23:59:55+00:00</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Twitter for Android</td>\n","      <td>550441111513493504</td>\n","      <td>\"@gillule4: @realDonaldTrump incredible experi...</td>\n","      <td>2014-12-31 23:59:22+00:00</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>18</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Twitter for Android</td>\n","      <td>550440752254562304</td>\n","      <td>\"@JobSnarechs: Negotiation tip #1: The worst t...</td>\n","      <td>2014-12-31 23:57:56+00:00</td>\n","      <td>33</td>\n","      <td>NaN</td>\n","      <td>44</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Twitter for Android</td>\n","      <td>550440620792492032</td>\n","      <td>\"@joelmch2os: @realDonaldTrump announce your p...</td>\n","      <td>2014-12-31 23:57:25+00:00</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>26</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Twitter for Android</td>\n","      <td>550440523094577152</td>\n","      <td>\"@djspookyshadow: Feeling a deep gratitude for...</td>\n","      <td>2014-12-31 23:57:02+00:00</td>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>31</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>23655</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950946293077209088</td>\n","      <td>.@ICEgov HSI agents and ERO officers, on behal...</td>\n","      <td>2018-01-10 04:23:58+00:00</td>\n","      <td>13822</td>\n","      <td>NaN</td>\n","      <td>60252</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>23656</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950937350003183616</td>\n","      <td>Thank you @GOPLeader Kevin McCarthy! Couldn’t ...</td>\n","      <td>2018-01-10 03:48:26+00:00</td>\n","      <td>11893</td>\n","      <td>NaN</td>\n","      <td>51951</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>23657</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950884128379035648</td>\n","      <td>As I made very clear today, our country needs ...</td>\n","      <td>2018-01-10 00:16:57+00:00</td>\n","      <td>24552</td>\n","      <td>NaN</td>\n","      <td>123413</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>23658</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950879772644659200</td>\n","      <td>RT @Scavino45: “Utilities cutting rates, cite ...</td>\n","      <td>2018-01-09 23:59:38+00:00</td>\n","      <td>11716</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>23659</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950866561153331200</td>\n","      <td>Today, it was my great honor to sign a new Exe...</td>\n","      <td>2018-01-09 23:07:09+00:00</td>\n","      <td>19481</td>\n","      <td>NaN</td>\n","      <td>82151</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>23660 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb3178fe-70ab-44ff-82f0-b120bc652e58')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fb3178fe-70ab-44ff-82f0-b120bc652e58 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fb3178fe-70ab-44ff-82f0-b120bc652e58');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cc861810-ba92-487d-b9b2-b0c3e439acdc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc861810-ba92-487d-b9b2-b0c3e439acdc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cc861810-ba92-487d-b9b2-b0c3e439acdc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_2a983216-ce1c-4821-ba40-dfd580d68069\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_2a983216-ce1c-4821-ba40-dfd580d68069 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 23660,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Twitter for Android\",\n          \"Twitter Web Client\",\n          \"Periscope\"\n        ],\n        \"num_unique_values\": 18,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184793163017284384,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"samples\": [\n          888708453560184832,\n          622621762023518208,\n          507163439366934528\n        ],\n        \"num_unique_values\": 23660,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Does anyone notice how the Montana Congressional race was such a big deal to Dems &amp; Fake News until the Republican won? V was poorly covered\",\n          \"\\\"@ryan5240 after playing the new course in Aberdeen and loved it my wife taking me to Turnberry for a birthday present on Monday\\\" Great!\",\n          \"\\\"@JonathanAFrye: Passion is the start; confidence, vision &amp; focus are the steps, &amp; resilience is the last mile to entrepreneurial success.\\\"\"\n        ],\n        \"num_unique_values\": 23562,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"samples\": [\n          \"2018-09-03 11:05:37+00:00\",\n          \"2015-02-23 20:29:40+00:00\",\n          \"2016-10-06 01:53:05+00:00\"\n        ],\n        \"num_unique_values\": 23632,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10960,\n        \"min\": 0,\n        \"max\": 369530,\n        \"samples\": [\n          10345,\n          31318,\n          226\n        ],\n        \"num_unique_values\": 10340,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 528705872.62160987,\n        \"min\": 759251.0,\n        \"max\": 3412873425.0,\n        \"samples\": [\n          633797941.0,\n          56411751.0,\n          23970102.0\n        ],\n        \"num_unique_values\": 90,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40767,\n        \"min\": 0,\n        \"max\": 633253,\n        \"samples\": [\n          1404,\n          133250,\n          124746\n        ],\n        \"num_unique_values\": 11909,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"samples\": [\n          true,\n          false\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":51}],"source":["df.reset_index(drop=True, inplace=True)\n","df"]},{"cell_type":"markdown","metadata":{"id":"_OwrXW9diHDT"},"source":["Trump used to live on Twitter lol.\n","\n","Here, we borrow the idea of the information retrieval algorithm. We define the cosine similarity of the tweet's vector representation to the target post's vector representation as relevance, the normalized score (popularity) as importance, and normalized days apart from the debate day as recency. Then, we pick top 5 comments for the Trump agent to learn.  "]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":13172,"status":"ok","timestamp":1708286751621,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"},"user_tz":360},"id":"P3Nha6jPiDlM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"35d5ebac-3324-46b0-d5ec-376f13198eb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence_transformers\n","  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/132.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/132.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Installing collected packages: sentence_transformers\n","Successfully installed sentence_transformers-2.3.1\n"]}],"source":["!pip install sentence_transformers\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"cSDvxjQil5_a","executionInfo":{"status":"ok","timestamp":1708286925376,"user_tz":360,"elapsed":16435,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["# relevance\n","embedding_model_name = 'all-MiniLM-L6-v2'\n","model = SentenceTransformer(embedding_model_name, device='cuda')\n","embeddings = model.encode(df['text'].astype(str))\n","target_question = 'Why is your position correct on selecting a Supreme Court nominee in an election year?'\n","similarity = cosine_similarity(embeddings, model.encode([target_question]))\n","df['cosine_similarity'] = similarity\n","df['relevance'] = (df['cosine_similarity'] - df['cosine_similarity'].min()) / (df['cosine_similarity'].max() - df['cosine_similarity'].min())"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"KQdddSTwmWFo","executionInfo":{"status":"ok","timestamp":1708286935044,"user_tz":360,"elapsed":239,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["# recency\n","dates = df['created_at'].dt.date\n","target_date = '2020-09-29'\n","df['days_diff'] = (pd.to_datetime(target_date) - pd.to_datetime(dates)).dt.days\n","df['days_diff'] = 0.99 ** df['days_diff']\n","df['recency'] = (df['days_diff'] - df['days_diff'].min()) / (df['days_diff'].max() - df['days_diff'].min())"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"8YRjCNsImiNi","executionInfo":{"status":"ok","timestamp":1708286935929,"user_tz":360,"elapsed":16,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["# importance\n","scores = df['favorite_count']\n","df['importance'] = (scores - scores.min()) / (scores.max() - scores.min())"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":825},"id":"LgYcmPkdzIBt","executionInfo":{"status":"ok","timestamp":1708286936912,"user_tz":360,"elapsed":26,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"fdda9724-53b3-4a28-9aca-58b88631a9ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    source              id_str  \\\n","0      Twitter for Android  550441250965708800   \n","1      Twitter for Android  550441111513493504   \n","2      Twitter for Android  550440752254562304   \n","3      Twitter for Android  550440620792492032   \n","4      Twitter for Android  550440523094577152   \n","...                    ...                 ...   \n","23655   Twitter for iPhone  950946293077209088   \n","23656   Twitter for iPhone  950937350003183616   \n","23657   Twitter for iPhone  950884128379035648   \n","23658   Twitter for iPhone  950879772644659200   \n","23659   Twitter for iPhone  950866561153331200   \n","\n","                                                    text  \\\n","0      \"@ronmeier123: @Macys Your APPAREL is UNPARALL...   \n","1      \"@gillule4: @realDonaldTrump incredible experi...   \n","2      \"@JobSnarechs: Negotiation tip #1: The worst t...   \n","3      \"@joelmch2os: @realDonaldTrump announce your p...   \n","4      \"@djspookyshadow: Feeling a deep gratitude for...   \n","...                                                  ...   \n","23655  .@ICEgov HSI agents and ERO officers, on behal...   \n","23656  Thank you @GOPLeader Kevin McCarthy! Couldn’t ...   \n","23657  As I made very clear today, our country needs ...   \n","23658  RT @Scavino45: “Utilities cutting rates, cite ...   \n","23659  Today, it was my great honor to sign a new Exe...   \n","\n","                     created_at  retweet_count  in_reply_to_user_id_str  \\\n","0     2014-12-31 23:59:55+00:00              8                      NaN   \n","1     2014-12-31 23:59:22+00:00              5                      NaN   \n","2     2014-12-31 23:57:56+00:00             33                      NaN   \n","3     2014-12-31 23:57:25+00:00              8                      NaN   \n","4     2014-12-31 23:57:02+00:00              9                      NaN   \n","...                         ...            ...                      ...   \n","23655 2018-01-10 04:23:58+00:00          13822                      NaN   \n","23656 2018-01-10 03:48:26+00:00          11893                      NaN   \n","23657 2018-01-10 00:16:57+00:00          24552                      NaN   \n","23658 2018-01-09 23:59:38+00:00          11716                      NaN   \n","23659 2018-01-09 23:07:09+00:00          19481                      NaN   \n","\n","       favorite_count  is_retweet  cosine_similarity  relevance     days_diff  \\\n","0                  21       False           0.154324   0.463599  6.890861e-10   \n","1                  18       False           0.106285   0.403678  6.890861e-10   \n","2                  44       False           0.100925   0.396992  6.890861e-10   \n","3                  26       False           0.133932   0.438164  6.890861e-10   \n","4                  31       False           0.013051   0.287382  6.890861e-10   \n","...               ...         ...                ...        ...           ...   \n","23655           60252       False           0.084336   0.376299  4.631783e-05   \n","23656           51951       False           0.011486   0.285431  4.631783e-05   \n","23657          123413       False           0.079299   0.370017  4.631783e-05   \n","23658               0        True           0.079789   0.370628  4.585465e-05   \n","23659           82151       False           0.062218   0.348710  4.585465e-05   \n","\n","            recency  importance  \n","0      4.089576e-07    0.000033  \n","1      4.089576e-07    0.000028  \n","2      4.089576e-07    0.000069  \n","3      4.089576e-07    0.000041  \n","4      4.089576e-07    0.000049  \n","...             ...         ...  \n","23655  2.821590e-02    0.095147  \n","23656  2.821590e-02    0.082038  \n","23657  2.821590e-02    0.194887  \n","23658  2.793374e-02    0.000000  \n","23659  2.793374e-02    0.129729  \n","\n","[23660 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-86d21192-e526-4f62-9e60-da3d8a7a080b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>id_str</th>\n","      <th>text</th>\n","      <th>created_at</th>\n","      <th>retweet_count</th>\n","      <th>in_reply_to_user_id_str</th>\n","      <th>favorite_count</th>\n","      <th>is_retweet</th>\n","      <th>cosine_similarity</th>\n","      <th>relevance</th>\n","      <th>days_diff</th>\n","      <th>recency</th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Twitter for Android</td>\n","      <td>550441250965708800</td>\n","      <td>\"@ronmeier123: @Macys Your APPAREL is UNPARALL...</td>\n","      <td>2014-12-31 23:59:55+00:00</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>False</td>\n","      <td>0.154324</td>\n","      <td>0.463599</td>\n","      <td>6.890861e-10</td>\n","      <td>4.089576e-07</td>\n","      <td>0.000033</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Twitter for Android</td>\n","      <td>550441111513493504</td>\n","      <td>\"@gillule4: @realDonaldTrump incredible experi...</td>\n","      <td>2014-12-31 23:59:22+00:00</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>18</td>\n","      <td>False</td>\n","      <td>0.106285</td>\n","      <td>0.403678</td>\n","      <td>6.890861e-10</td>\n","      <td>4.089576e-07</td>\n","      <td>0.000028</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Twitter for Android</td>\n","      <td>550440752254562304</td>\n","      <td>\"@JobSnarechs: Negotiation tip #1: The worst t...</td>\n","      <td>2014-12-31 23:57:56+00:00</td>\n","      <td>33</td>\n","      <td>NaN</td>\n","      <td>44</td>\n","      <td>False</td>\n","      <td>0.100925</td>\n","      <td>0.396992</td>\n","      <td>6.890861e-10</td>\n","      <td>4.089576e-07</td>\n","      <td>0.000069</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Twitter for Android</td>\n","      <td>550440620792492032</td>\n","      <td>\"@joelmch2os: @realDonaldTrump announce your p...</td>\n","      <td>2014-12-31 23:57:25+00:00</td>\n","      <td>8</td>\n","      <td>NaN</td>\n","      <td>26</td>\n","      <td>False</td>\n","      <td>0.133932</td>\n","      <td>0.438164</td>\n","      <td>6.890861e-10</td>\n","      <td>4.089576e-07</td>\n","      <td>0.000041</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Twitter for Android</td>\n","      <td>550440523094577152</td>\n","      <td>\"@djspookyshadow: Feeling a deep gratitude for...</td>\n","      <td>2014-12-31 23:57:02+00:00</td>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>31</td>\n","      <td>False</td>\n","      <td>0.013051</td>\n","      <td>0.287382</td>\n","      <td>6.890861e-10</td>\n","      <td>4.089576e-07</td>\n","      <td>0.000049</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>23655</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950946293077209088</td>\n","      <td>.@ICEgov HSI agents and ERO officers, on behal...</td>\n","      <td>2018-01-10 04:23:58+00:00</td>\n","      <td>13822</td>\n","      <td>NaN</td>\n","      <td>60252</td>\n","      <td>False</td>\n","      <td>0.084336</td>\n","      <td>0.376299</td>\n","      <td>4.631783e-05</td>\n","      <td>2.821590e-02</td>\n","      <td>0.095147</td>\n","    </tr>\n","    <tr>\n","      <th>23656</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950937350003183616</td>\n","      <td>Thank you @GOPLeader Kevin McCarthy! Couldn’t ...</td>\n","      <td>2018-01-10 03:48:26+00:00</td>\n","      <td>11893</td>\n","      <td>NaN</td>\n","      <td>51951</td>\n","      <td>False</td>\n","      <td>0.011486</td>\n","      <td>0.285431</td>\n","      <td>4.631783e-05</td>\n","      <td>2.821590e-02</td>\n","      <td>0.082038</td>\n","    </tr>\n","    <tr>\n","      <th>23657</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950884128379035648</td>\n","      <td>As I made very clear today, our country needs ...</td>\n","      <td>2018-01-10 00:16:57+00:00</td>\n","      <td>24552</td>\n","      <td>NaN</td>\n","      <td>123413</td>\n","      <td>False</td>\n","      <td>0.079299</td>\n","      <td>0.370017</td>\n","      <td>4.631783e-05</td>\n","      <td>2.821590e-02</td>\n","      <td>0.194887</td>\n","    </tr>\n","    <tr>\n","      <th>23658</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950879772644659200</td>\n","      <td>RT @Scavino45: “Utilities cutting rates, cite ...</td>\n","      <td>2018-01-09 23:59:38+00:00</td>\n","      <td>11716</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>0.079789</td>\n","      <td>0.370628</td>\n","      <td>4.585465e-05</td>\n","      <td>2.793374e-02</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>23659</th>\n","      <td>Twitter for iPhone</td>\n","      <td>950866561153331200</td>\n","      <td>Today, it was my great honor to sign a new Exe...</td>\n","      <td>2018-01-09 23:07:09+00:00</td>\n","      <td>19481</td>\n","      <td>NaN</td>\n","      <td>82151</td>\n","      <td>False</td>\n","      <td>0.062218</td>\n","      <td>0.348710</td>\n","      <td>4.585465e-05</td>\n","      <td>2.793374e-02</td>\n","      <td>0.129729</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>23660 rows × 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86d21192-e526-4f62-9e60-da3d8a7a080b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-86d21192-e526-4f62-9e60-da3d8a7a080b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-86d21192-e526-4f62-9e60-da3d8a7a080b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-da5054d5-10cd-42de-9417-189fad7d38f5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da5054d5-10cd-42de-9417-189fad7d38f5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-da5054d5-10cd-42de-9417-189fad7d38f5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_a09336ca-5c25-46e2-9f75-b19e8fbe2819\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a09336ca-5c25-46e2-9f75-b19e8fbe2819 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 23660,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Twitter for Android\",\n          \"Twitter Web Client\",\n          \"Periscope\"\n        ],\n        \"num_unique_values\": 18,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184793163017284384,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"samples\": [\n          888708453560184832,\n          622621762023518208,\n          507163439366934528\n        ],\n        \"num_unique_values\": 23660,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Does anyone notice how the Montana Congressional race was such a big deal to Dems &amp; Fake News until the Republican won? V was poorly covered\",\n          \"\\\"@ryan5240 after playing the new course in Aberdeen and loved it my wife taking me to Turnberry for a birthday present on Monday\\\" Great!\",\n          \"\\\"@JonathanAFrye: Passion is the start; confidence, vision &amp; focus are the steps, &amp; resilience is the last mile to entrepreneurial success.\\\"\"\n        ],\n        \"num_unique_values\": 23562,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"samples\": [\n          \"2018-09-03 11:05:37+00:00\",\n          \"2015-02-23 20:29:40+00:00\",\n          \"2016-10-06 01:53:05+00:00\"\n        ],\n        \"num_unique_values\": 23632,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10960,\n        \"min\": 0,\n        \"max\": 369530,\n        \"samples\": [\n          10345,\n          31318,\n          226\n        ],\n        \"num_unique_values\": 10340,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 528705872.62160987,\n        \"min\": 759251.0,\n        \"max\": 3412873425.0,\n        \"samples\": [\n          633797941.0,\n          56411751.0,\n          23970102.0\n        ],\n        \"num_unique_values\": 90,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40767,\n        \"min\": 0,\n        \"max\": 633253,\n        \"samples\": [\n          1404,\n          133250,\n          124746\n        ],\n        \"num_unique_values\": 11909,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"samples\": [\n          true,\n          false\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_similarity\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"samples\": [\n          0.02025945857167244,\n          0.176217183470726\n        ],\n        \"num_unique_values\": 23556,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"samples\": [\n          0.5689869523048401,\n          0.527502179145813\n        ],\n        \"num_unique_values\": 23531,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00023472484932599312,\n        \"min\": 1.776169032077608e-11,\n        \"max\": 0.0016415501428270432,\n        \"samples\": [\n          0.00031581013785891707,\n          1.0700541909259004e-07\n        ],\n        \"num_unique_values\": 1789,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1429897544655533,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"samples\": [\n          0.19238530415551813,\n          6.517477338561921e-05\n        ],\n        \"num_unique_values\": 1789,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06437791653941481,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"samples\": [\n          0.0022171233298539446,\n          0.21042142713891604\n        ],\n        \"num_unique_values\": 11909,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":55}],"source":["df"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"GuzvOKSvm-sS","colab":{"base_uri":"https://localhost:8080/","height":573},"executionInfo":{"status":"ok","timestamp":1708286975409,"user_tz":360,"elapsed":197,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"c659d9a0-b49a-4532-df83-327d43c23d6b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   source               id_str  \\\n","20193  Twitter for iPhone  1077549301449060352   \n","21175  Twitter for iPhone  1045444544068812800   \n","20152  Twitter for iPhone  1079830267274108928   \n","20162  Twitter for iPhone  1079497288605683712   \n","20223  Twitter for iPhone  1076655729820471296   \n","\n","                                                    text  \\\n","20193                                   Merry Christmas!   \n","21175  Judge Kavanaugh showed America exactly why I n...   \n","20152  Heads of countries are calling wanting to know...   \n","20162  President and Mrs. Obama built/has a ten foot ...   \n","20223  Brett McGurk, who I do not know, was appointed...   \n","\n","                     created_at  retweet_count  in_reply_to_user_id_str  \\\n","20193 2018-12-25 12:59:08+00:00          89255                      NaN   \n","21175 2018-09-27 22:46:17+00:00          84180                      NaN   \n","20152 2018-12-31 20:02:52+00:00          21030                      NaN   \n","20162 2018-12-30 21:59:44+00:00          61837                      NaN   \n","20223 2018-12-23 01:48:23+00:00          19476                      NaN   \n","\n","       favorite_count  is_retweet  cosine_similarity  relevance  days_diff  \\\n","20193          508372       False          -0.025833   0.238880   0.001545   \n","21175          320104       False           0.509458   0.906575   0.000632   \n","20152           76721       False           0.292551   0.636017   0.001642   \n","20162          233722       False           0.097907   0.393227   0.001625   \n","20223           86619       False           0.334501   0.688343   0.001515   \n","\n","        recency  importance     score  \n","20193  0.941480    0.802794  1.983155  \n","21175  0.384896    0.505491  1.796962  \n","20152  1.000000    0.121154  1.757171  \n","20162  0.990000    0.369082  1.752309  \n","20223  0.922745    0.136784  1.747871  "],"text/html":["\n","  <div id=\"df-525fbefd-fb8d-4b80-abba-d30aea819e27\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>id_str</th>\n","      <th>text</th>\n","      <th>created_at</th>\n","      <th>retweet_count</th>\n","      <th>in_reply_to_user_id_str</th>\n","      <th>favorite_count</th>\n","      <th>is_retweet</th>\n","      <th>cosine_similarity</th>\n","      <th>relevance</th>\n","      <th>days_diff</th>\n","      <th>recency</th>\n","      <th>importance</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>20193</th>\n","      <td>Twitter for iPhone</td>\n","      <td>1077549301449060352</td>\n","      <td>Merry Christmas!</td>\n","      <td>2018-12-25 12:59:08+00:00</td>\n","      <td>89255</td>\n","      <td>NaN</td>\n","      <td>508372</td>\n","      <td>False</td>\n","      <td>-0.025833</td>\n","      <td>0.238880</td>\n","      <td>0.001545</td>\n","      <td>0.941480</td>\n","      <td>0.802794</td>\n","      <td>1.983155</td>\n","    </tr>\n","    <tr>\n","      <th>21175</th>\n","      <td>Twitter for iPhone</td>\n","      <td>1045444544068812800</td>\n","      <td>Judge Kavanaugh showed America exactly why I n...</td>\n","      <td>2018-09-27 22:46:17+00:00</td>\n","      <td>84180</td>\n","      <td>NaN</td>\n","      <td>320104</td>\n","      <td>False</td>\n","      <td>0.509458</td>\n","      <td>0.906575</td>\n","      <td>0.000632</td>\n","      <td>0.384896</td>\n","      <td>0.505491</td>\n","      <td>1.796962</td>\n","    </tr>\n","    <tr>\n","      <th>20152</th>\n","      <td>Twitter for iPhone</td>\n","      <td>1079830267274108928</td>\n","      <td>Heads of countries are calling wanting to know...</td>\n","      <td>2018-12-31 20:02:52+00:00</td>\n","      <td>21030</td>\n","      <td>NaN</td>\n","      <td>76721</td>\n","      <td>False</td>\n","      <td>0.292551</td>\n","      <td>0.636017</td>\n","      <td>0.001642</td>\n","      <td>1.000000</td>\n","      <td>0.121154</td>\n","      <td>1.757171</td>\n","    </tr>\n","    <tr>\n","      <th>20162</th>\n","      <td>Twitter for iPhone</td>\n","      <td>1079497288605683712</td>\n","      <td>President and Mrs. Obama built/has a ten foot ...</td>\n","      <td>2018-12-30 21:59:44+00:00</td>\n","      <td>61837</td>\n","      <td>NaN</td>\n","      <td>233722</td>\n","      <td>False</td>\n","      <td>0.097907</td>\n","      <td>0.393227</td>\n","      <td>0.001625</td>\n","      <td>0.990000</td>\n","      <td>0.369082</td>\n","      <td>1.752309</td>\n","    </tr>\n","    <tr>\n","      <th>20223</th>\n","      <td>Twitter for iPhone</td>\n","      <td>1076655729820471296</td>\n","      <td>Brett McGurk, who I do not know, was appointed...</td>\n","      <td>2018-12-23 01:48:23+00:00</td>\n","      <td>19476</td>\n","      <td>NaN</td>\n","      <td>86619</td>\n","      <td>False</td>\n","      <td>0.334501</td>\n","      <td>0.688343</td>\n","      <td>0.001515</td>\n","      <td>0.922745</td>\n","      <td>0.136784</td>\n","      <td>1.747871</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-525fbefd-fb8d-4b80-abba-d30aea819e27')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-525fbefd-fb8d-4b80-abba-d30aea819e27 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-525fbefd-fb8d-4b80-abba-d30aea819e27');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d6d12b67-6ba5-498c-b092-04995c9d38bf\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6d12b67-6ba5-498c-b092-04995c9d38bf')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d6d12b67-6ba5-498c-b092-04995c9d38bf button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 23660,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Twitter for iPhone\",\n          \"Twitter Media Studio\",\n          \"Instagram\"\n        ],\n        \"num_unique_values\": 18,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184793163017284384,\n        \"min\": 418365112844824576,\n        \"max\": 1079888205351145472,\n        \"samples\": [\n          739934090690998272,\n          758236300554174464,\n          1029908960265027584\n        ],\n        \"num_unique_values\": 23660,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Packed venue of people who want to #MakeAmericaGreatAgain https://t.co/wisZRVAmPR\",\n          \"August job openings hit a record 7.14 million. Congratulations USA!\",\n          \"\\\"@jackievandyck: @realDonaldTrump I get to vote for the president for the first time in 2016. I hope you're on the ballot\\\"\"\n        ],\n        \"num_unique_values\": 23562,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-01 12:56:30+00:00\",\n        \"max\": \"2018-12-31 23:53:06+00:00\",\n        \"samples\": [\n          \"2014-01-06 21:42:43+00:00\",\n          \"2016-03-19 15:40:55+00:00\",\n          \"2016-02-23 05:19:29+00:00\"\n        ],\n        \"num_unique_values\": 23632,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10960,\n        \"min\": 0,\n        \"max\": 369530,\n        \"samples\": [\n          11231,\n          1426,\n          11577\n        ],\n        \"num_unique_values\": 10340,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in_reply_to_user_id_str\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 528705872.62160987,\n        \"min\": 759251.0,\n        \"max\": 3412873425.0,\n        \"samples\": [\n          357606935.0,\n          3412873425.0,\n          897350694.0\n        ],\n        \"num_unique_values\": 90,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40767,\n        \"min\": 0,\n        \"max\": 633253,\n        \"samples\": [\n          60786,\n          4730,\n          9441\n        ],\n        \"num_unique_values\": 11909,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_retweet\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"samples\": [\n          true,\n          false\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_similarity\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"samples\": [\n          -0.02946225181221962,\n          0.12436496466398239\n        ],\n        \"num_unique_values\": 23556,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"samples\": [\n          0.3003414571285248,\n          0.4468393623828888\n        ],\n        \"num_unique_values\": 23531,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00023472484932599312,\n        \"min\": 1.776169032077608e-11,\n        \"max\": 0.0016415501428270432,\n        \"samples\": [\n          4.2581638022099654e-11,\n          1.321498534163712e-07\n        ],\n        \"num_unique_values\": 1789,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1429897544655533,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"samples\": [\n          1.511982322217267e-08,\n          8.049226746627064e-05\n        ],\n        \"num_unique_values\": 1789,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06437791653941481,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"samples\": [\n          0.09599007031944579,\n          0.007469368483054956\n        ],\n        \"num_unique_values\": 11909,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20476041542898807,\n        \"min\": 0.0013356809063603486,\n        \"max\": 1.9831550341777273,\n        \"samples\": [\n          0.28642171399412325,\n          0.7166079397045703\n        ],\n        \"num_unique_values\": 23657,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":56}],"source":["# select top 5\n","df['score'] = df['relevance'] + df['recency'] + df['importance']\n","df = df.sort_values(by='score', ascending=False)\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"NsFEL0eczIBt"},"source":["Let's put the top 5 twwets into the prompt (Biden) and re-run the code"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"3ankzgBhzIBt","executionInfo":{"status":"ok","timestamp":1708286992056,"user_tz":360,"elapsed":120,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"Jt7RzfPEzIBt","executionInfo":{"status":"ok","timestamp":1708286992465,"user_tz":360,"elapsed":14,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["from langchain.llms import OpenAI\n","\n","llm_openai = OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo-instruct\", max_tokens=512)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cciqx5UbzIBt","executionInfo":{"status":"ok","timestamp":1708286993623,"user_tz":360,"elapsed":88,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"fbcb99f2-7adb-4c87-fb84-f57e3fd4a9f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Merry Christmas!',\n"," 'Judge Kavanaugh showed America exactly why I nominated him. His testimony was powerful, honest, and riveting. Democrats’ search and destroy strategy is disgraceful and this process has been a total sham and effort to delay, obstruct, and resist. The Senate must vote!',\n"," 'Heads of countries are calling wanting to know why Senator Schumer is not approving their otherwise approved Ambassadors!? Likewise in Government lawyers and others are being delayed at a record pace! 360 great and hardworking people are waiting for approval from....',\n"," 'President and Mrs. Obama built/has a ten foot Wall around their D.C. mansion/compound. I agree, totally necessary for their safety and security. The U.S. needs the same thing, slightly larger version!',\n"," 'Brett McGurk, who I do not know, was appointed by President Obama in 2015. Was supposed to leave in February but he just resigned prior to leaving. Grandstander? The Fake News is making such a big deal about this nothing event!']"]},"metadata":{},"execution_count":59}],"source":["df.head(5)['text'].tolist()"]},{"cell_type":"markdown","metadata":{"id":"0ZHDjsdxzIBt"},"source":["Great, we saw Judge Kavanaugh appeared and topics like border walls."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"yFrIHTXpzIBt","executionInfo":{"status":"ok","timestamp":1708287306387,"user_tz":360,"elapsed":140,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["Trump_memory = ConversationBufferMemory(human_prefix='Trump', ai_prefix='Biden')\n","Biden_memory = ConversationBufferMemory(human_prefix='Biden', ai_prefix='Trump')\n","\n","Trump_template = \"\"\"Imagine you're the Democrats presidential candidate Joe Biden. Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Donald Trump. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n","\n","Current conversation:\n","{history}\n","Trump: {input}\n","Biden:\"\"\"\n","\n","tweets = df.head(5)['text'].tolist()\n","Biden_template = \"\"\"\"Imagine you're the Republican presidential candidate Donald Trump. To give you a better sense of how Trump may give public comments, here're some of his tweets:\n","    1. Merry Christmas!\n","    2. Judge Kavanaugh showed America exactly why I nominated him. His testimony was powerful, honest, and riveting. Democrats’ search and destroy strategy is disgraceful and this process has been a total sham and effort to delay, obstruct, and resist. The Senate must vote!\n","    3. Heads of countries are calling wanting to know why Senator Schumer is not approving their otherwise approved Ambassadors!? Likewise in Government lawyers and others are being delayed at a record pace! 360 great and hardworking people are waiting for approval from....\n","    4. President and Mrs. Obama built/has a ten foot Wall around their D.C. mansion/compound. I agree, totally necessary for their safety and security. The U.S. needs the same thing, slightly larger version!\n","    5. Brett McGurk, who I do not know, was appointed by President Obama in 2015. Was supposed to leave in February but he just resigned prior to leaving. Grandstander? The Fake News is making such a big deal about this nothing event!\n","Today is Tuesday, September 29, 2020, and you're engaged in your first television presidential debate with Joe Biden. The deabte question is: Why is your position correct on selecting a Supreme Court nominee in an election year?\n","\n","Current conversation:\n","{history}\n","Biden: {input}\n","Trump:\"\"\"\n","\n","# By default, models recognize themselves as AIs. So we need to let them do role play\n","# and convince them that they're talking to real Trump/Biden (though they're not)\n","\n","TRUMP_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Trump_template)\n","BIDEN_PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=Biden_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzDEECwUzIBt"},"outputs":[],"source":["# Trump_conversation = ConversationChain(\n","#     prompt=TRUMP_PROMPT,\n","#     llm=llm_openai,\n","#     verbose=False,\n","#     memory=Trump_memory\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11cJDrtNzIBt"},"outputs":[],"source":["# suppose Biden starts first\n","# biden_response = Trump_conversation.predict(input='')"]},{"cell_type":"code","source":["# use the same biden response\n","biden_response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"dhCvJ_DD3G9L","executionInfo":{"status":"ok","timestamp":1708287360620,"user_tz":360,"elapsed":98,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"e941403f-a6dd-4256-9a17-bf288fb10938"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\"Well, first of all, I want to thank everyone for tuning in tonight. This is an important discussion about the future of our country and the Supreme Court is a crucial part of that. Now, the question at hand is about selecting a Supreme Court nominee in an election year and let me be clear, my position is not only the correct one, it\\'s the only one that honors the American people and our constitution.\\n\\nLet\\'s start with the facts. In 2016, President Obama nominated Merrick Garland to fill the vacant Supreme Court seat. But the Republican-controlled Senate refused to even hold a hearing, let alone a vote. And now, just four years later, these same Republicans are rushing to fill the seat left by the late Justice Ruth Bader Ginsburg. This blatant hypocrisy and disregard for the will of the American people is unacceptable.\\n\\nThe American people are already voting. Millions of ballots have been cast and millions more will be cast before Election Day. In fact, in 2016, Senate Majority Leader McConnell argued that the American people should have a say in the selection of the next Supreme Court Justice and that the nomination should wait until after the election. I couldn\\'t agree more. The American people deserve to have a voice in this process and that voice is their vote.\\n\\nThis is not about Democrats or Republicans, this is about the integrity of our democracy. Rushing a nominee through in the midst of an election is a dangerous precedent and it undermines the legitimacy of the Supreme Court. The American people deserve a Supreme Court that is fair and impartial, not one that is stacked with partisan justices.\\n\\nSo, my position is clear. As President, I would honor the will of the American people and wait until after the election to nominate a Supreme Court Justice. This is the only way to ensure that the American people have a say in the future of the court and that our democracy remains strong. Thank you.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}]},{"cell_type":"code","execution_count":62,"metadata":{"id":"QE9HEK2UzIBt","executionInfo":{"status":"ok","timestamp":1708287406797,"user_tz":360,"elapsed":112,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["Biden_conversation = ConversationChain(\n","    prompt=BIDEN_PROMPT,\n","    llm=llm_openai,\n","    verbose=False,\n","    memory=Biden_memory\n",")"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"lJ7GEXuDzIBt","executionInfo":{"status":"ok","timestamp":1708287411481,"user_tz":360,"elapsed":3919,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}}},"outputs":[],"source":["trump_response = Biden_conversation.predict(input=biden_response)"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"MZoIG56hzIBt","executionInfo":{"status":"ok","timestamp":1708287412672,"user_tz":360,"elapsed":133,"user":{"displayName":"Shiba Mao","userId":"16549320756357564965"}},"outputId":"0760cc3a-3772-45ac-9aeb-45c23490d671"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["' \"Well, let me start off by saying that this is just typical Joe Biden, playing politics and trying to deflect from his weak record. The fact is, as President, it is my duty to nominate a Supreme Court Justice and the Senate\\'s duty to confirm that nominee. And I will fulfill that duty.\\n\\nBut let\\'s talk about the hypocrisy here. The Democrats were the ones who started this whole thing back in 2016. They were the ones who blocked Judge Garland\\'s nomination. And now they want to cry foul when the tables are turned? Give me a break.\\n\\nThe American people elected me to make these decisions and that\\'s exactly what I\\'m going to do. We have a vacancy on the Supreme Court and it needs to be filled as soon as possible. The American people deserve a full court and I will not let politics get in the way of that.\\n\\nAnd let\\'s talk about the timing. The truth is, there\\'s never a good time for a Supreme Court vacancy. But as President, I have a responsibility to act and I will not shirk that responsibility just because it happens to be an election year.\\n\\nFurthermore, I want to address the media\\'s false narrative about the rush to fill this seat. The fact is, there have been 29 Supreme Court nominations in an election year and 22 of those have been confirmed. So, this is not some unprecedented move, despite what the fake news media may want you to believe.\\n\\nIn conclusion, my position is simple. As President, I will fulfill my duty to nominate a Supreme Court Justice and the Senate will fulfill their duty to confirm that nominee. That\\'s how our system of checks and balances works and that\\'s what the American people expect and deserve.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}],"source":["trump_response"]},{"cell_type":"markdown","source":["Interestingly, the agent's response sounds more like Trump by starting with a harsh response to Biden's statement."],"metadata":{"id":"-cb2qr4I3jG2"}},{"cell_type":"markdown","metadata":{"id":"LkoPr9CQu4T2"},"source":["## <font color=\"red\">*Exercise 4*</font>\n","\n","<font color=\"red\">Use LangChain(you're welcome to not use it) to set up conversations with LLM agents for questions related to your final project (if relevant), or think of a scenario that a simulated conversation could be useful to answer a research question and find a dataset to implement it. What does it reveal about the social game involved with your dataset?\n","\n","<font color=\"red\"> Stretch: Use the idea of memory retrieval(or other methods) to design better templates for the LLM conversation."]},{"cell_type":"code","source":[],"metadata":{"id":"cRQvTtPZ442g"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"gpuType":"V100","provenance":[{"file_id":"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks/blob/main/week-8/8-Conversation-and-Text-Generation.ipynb","timestamp":1708201322949}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}